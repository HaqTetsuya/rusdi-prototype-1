{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaqTetsuya/ChatbotPerpusBipa/blob/main/IndobertPerpusChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baq_vbCGocRh",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title download dependancy\n",
        "# Cell 1: Instalasi library yang diperlukan\n",
        "!pip install transformers torch pandas scikit-learn matplotlib seaborn tqdm deep-translator fuzzywuzzy Levenshtein\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-8eolkt8hn4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19ce7ce-d66c-4614-d21e-52425db7aab8",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ChatbotPerpusBipa' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title import dependency, load drive, and github {\"form-width\":\"20%\"}\n",
        "!git clone https://github.com/HaqTetsuya/ChatbotPerpusBipa.git\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from google.colab import drive, files\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm  # If you need both tqdm and tqdm.auto\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "FName = \"RusdiIntents\" #@param {type:\"string\"}\n",
        "\n",
        "# Update MODEL_SAVE_PATH with user input\n",
        "MODEL_SAVE_PATH = f\"/content/drive/MyDrive/{FName}\"\n",
        "\n",
        "\n",
        "# Cell 4: Kelas Dataset untuk IndoBERT\n",
        "class IntentDataset(Dataset):\n",
        "    \"\"\"Dataset untuk klasifikasi intent dengan IndoBERT\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert dict of tensors to flat tensors\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(label)\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qqQ8ahtVzC6r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title load data\n",
        "\n",
        "def load_csv_data(csv_path, label_encoder=None, show_distribution=False):\n",
        "    \"\"\"Memuat data intent dari file CSV. Bisa untuk train/test tanpa split.\"\"\"\n",
        "    print(f\"\\nMemuat data dari: {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"File tidak ditemukan: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'text' not in df.columns or 'intent' not in df.columns:\n",
        "        raise ValueError(\"Kolom 'text' dan 'intent' harus ada di CSV\")\n",
        "\n",
        "    if label_encoder is None:\n",
        "        label_encoder = LabelEncoder()\n",
        "        df['intent_encoded'] = label_encoder.fit_transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Label encoder baru dibuat dari data {csv_path}\")\n",
        "    else:\n",
        "        df['intent_encoded'] = label_encoder.transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Menggunakan label encoder yang sudah ada\")\n",
        "\n",
        "    if show_distribution:\n",
        "        intent_counts = df['intent'].value_counts()\n",
        "        print(\"\\nDistribusi intent:\")\n",
        "        for intent, count in intent_counts.items():\n",
        "            print(f\"  {intent}: {count}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(x=intent_counts.index, y=intent_counts.values, palette=\"viridis\")\n",
        "        plt.xlabel(\"Intent\")\n",
        "        plt.ylabel(\"Jumlah Sampel\")\n",
        "        plt.title(\"Distribusi Intent\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.pie(intent_counts, labels=intent_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"viridis\", len(intent_counts)))\n",
        "        plt.title(\"Proporsi Intent\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    texts = df['text'].values\n",
        "    labels = df['intent_encoded'].values\n",
        "\n",
        "    return texts, labels, intent_classes, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "VMP8y1EczkWo"
      },
      "outputs": [],
      "source": [
        "# @title  setup model IndoBERT\n",
        "def setup_indobert_for_intent(num_labels):\n",
        "    \"\"\"Load model IndoBERT untuk klasifikasi intent\"\"\"\n",
        "\n",
        "    print(\"Memuat model IndoBERT...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"indobenchmark/indobert-base-p2\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    print(\"Model berhasil dimuat\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OOD\n",
        "def enhanced_calibrate_ood(model, tokenizer, val_dataloader, save_path, temperature=1.0, percentile=85, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate and save OOD thresholds with adjustable tolerance\n",
        "    \"\"\"\n",
        "    print(\"Calibrating threshold for OOD detection...\")\n",
        "    thresholds = calibrate_ood_detection(model, tokenizer, val_dataloader,\n",
        "                                        temperature=temperature,\n",
        "                                        percentile=percentile,\n",
        "                                        margin=margin)\n",
        "\n",
        "    print(f\"Energy threshold: {thresholds['energy_threshold']:.4f}\")\n",
        "    print(f\"MSP threshold: {thresholds['msp_threshold']:.4f}\")\n",
        "\n",
        "    # Save thresholds\n",
        "    save_ood_thresholds(thresholds, save_path)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def calibrate_ood_detection(model, tokenizer, dataloader, temperature=1.0, percentile=70, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate thresholds for OOD detection using in-distribution data\n",
        "    with Energy-based and MSP (Maximum Softmax Probability) methods\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # For Energy method and MSP method\n",
        "    energy_scores = []\n",
        "    msp_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Calibrating OOD detection\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Energy score (higher values for OOD)\n",
        "            energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "            energy_scores.extend(energy.cpu().numpy())\n",
        "\n",
        "            # MSP score (lower values for OOD)\n",
        "            softmax_probs = F.softmax(logits, dim=1)\n",
        "            max_probs, _ = torch.max(softmax_probs, dim=1)\n",
        "            msp_scores.extend(max_probs.cpu().numpy())\n",
        "\n",
        "    # Calculate threshold for Energy with margin (make more tolerant)\n",
        "    base_energy_threshold = np.percentile(energy_scores, percentile)\n",
        "    # Apply margin to make more tolerant (increase threshold)\n",
        "    energy_threshold = base_energy_threshold * (1 + margin)\n",
        "\n",
        "    # Calculate threshold for MSP with margin\n",
        "    base_msp_threshold = np.percentile(msp_scores, 100 - percentile)\n",
        "    # Apply margin to make more tolerant (decrease threshold)\n",
        "    msp_threshold = base_msp_threshold * (1 - margin)\n",
        "\n",
        "    return {\n",
        "        \"energy_threshold\": float(energy_threshold),\n",
        "        \"msp_threshold\": float(msp_threshold)\n",
        "    }\n",
        "\n",
        "def predict_with_ood_detection(model, tokenizer, text, thresholds, temperature=1.0, tolerance_factor=1.0):\n",
        "    \"\"\"\n",
        "    Predict with adjustable OOD detection tolerance\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Apply the tolerance factor to thresholds\n",
        "    energy_threshold = thresholds[\"energy_threshold\"] * tolerance_factor\n",
        "    msp_threshold = thresholds[\"msp_threshold\"] / tolerance_factor if thresholds[\"msp_threshold\"] else None\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Energy score\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "        energy_score = energy.item()\n",
        "\n",
        "        # MSP score\n",
        "        softmax_probs = F.softmax(logits, dim=1)\n",
        "        max_probs, predicted_class = torch.max(softmax_probs, dim=1)\n",
        "        msp_score = max_probs.item()\n",
        "\n",
        "        # OOD detection\n",
        "        is_ood_energy = energy_score > energy_threshold\n",
        "        is_ood_msp = msp_score < msp_threshold if msp_threshold else False\n",
        "\n",
        "        # Combined OOD detection (can adjust this logic for tolerance)\n",
        "        is_ood = is_ood_energy  # You can use different combinations\n",
        "\n",
        "        return {\n",
        "            \"prediction\": predicted_class.item(),\n",
        "            \"confidence\": msp_score,\n",
        "            \"energy_score\": energy_score,\n",
        "            \"is_ood\": is_ood,\n",
        "            \"is_ood_energy\": is_ood_energy,\n",
        "            \"is_ood_msp\": is_ood_msp\n",
        "        }\n",
        "\n",
        "def save_ood_thresholds(thresholds, save_path):\n",
        "    \"\"\"\n",
        "    Save OOD thresholds to JSON file\n",
        "    \"\"\"\n",
        "    threshold_file = os.path.join(save_path, \"ood_thresholds.json\")\n",
        "    with open(threshold_file, 'w') as f:\n",
        "        json.dump(thresholds, f, indent=4)\n",
        "    print(f\"OOD thresholds saved at {threshold_file}\")\n",
        "    return threshold_file\n",
        "\n",
        "def load_ood_thresholds(model_path):\n",
        "    \"\"\"\n",
        "    Load OOD thresholds from JSON file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(os.path.join(model_path, \"ood_thresholds.json\"), 'r') as f:\n",
        "            thresholds = json.load(f)\n",
        "            return thresholds\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            with open(os.path.join(model_path, \"ood_threshold.json\"), 'r') as f:\n",
        "                threshold_data = json.load(f)\n",
        "                return {\n",
        "                    \"energy_threshold\": threshold_data[\"energy_threshold\"],\n",
        "                    \"msp_threshold\": None\n",
        "                }\n",
        "        except FileNotFoundError:\n",
        "            print(\"Warning: OOD threshold files not found. Using default thresholds.\")\n",
        "            return {\n",
        "                \"energy_threshold\": 0.0,\n",
        "                \"msp_threshold\": 0.5\n",
        "            }"
      ],
      "metadata": {
        "id": "APTL6Ht96u9Q",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Focal LOSS\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n"
      ],
      "metadata": {
        "id": "VFbqGGCtvhG5",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fNCPJ3uFzp2V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "\n",
        "def train_intent_classifier(model, tokenizer, train_texts, train_labels, val_texts, val_labels,\n",
        "                           batch_size=16, epochs=10, learning_rate=2e-5, weight_decay=0.01,\n",
        "                           save_path=MODEL_SAVE_PATH, use_class_weights=True, patience=3, class_names=None):\n",
        "    \"\"\"\n",
        "    Melatih model IndoBERT untuk klasifikasi intent dengan perbaikan:\n",
        "    - Enhanced visualization (interactive and static)\n",
        "    - Per-class metric tracking\n",
        "    - Confusion matrix generation\n",
        "    - Batch-level metric tracking\n",
        "    - Learning rate visualization\n",
        "    \"\"\"\n",
        "\n",
        "    # Persiapkan dataset\n",
        "    print(\"Menyiapkan dataset...\")\n",
        "    train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Compute class weights if needed\n",
        "    class_weights = None\n",
        "    if use_class_weights:\n",
        "        unique_classes = np.unique(train_labels)\n",
        "        weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=unique_classes,\n",
        "            y=train_labels\n",
        "        )\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        class_weights = torch.FloatTensor(weights).to(device)\n",
        "        print(f\"Menggunakan class weights: {weights}\")\n",
        "\n",
        "    # Optimizer dengan weight decay untuk regularisasi\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Scheduler dengan warmup\n",
        "    num_training_steps = len(train_dataloader) * epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
        "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer,\n",
        "                             num_warmup_steps=num_warmup_steps,\n",
        "                             num_training_steps=num_training_steps)\n",
        "\n",
        "    # Cek untuk GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Menggunakan device: {device}\")\n",
        "    model.to(device)\n",
        "    print(f\"Mulai pelatihan model...\")\n",
        "    print(f\"Total epoch: {epochs}, batch size: {batch_size}, learning rate: {learning_rate}, weight decay: {weight_decay}\")\n",
        "\n",
        "    # Create loss function with class weights if needed\n",
        "    # Create Focal Loss with class weights\n",
        "    loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "    print(\"Menggunakan Focal Loss dengan gamma=2.0\")\n",
        "\n",
        "    # Initialize training history\n",
        "    history = initialize_training_history()\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0  # Counter for early stopping\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        train_loss = run_training_epoch(model, device, train_dataloader, optimizer,\n",
        "                                        scheduler, loss_fn, epoch, epochs, history)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs)\n",
        "        update_history_with_validation_metrics(history, val_metrics)\n",
        "\n",
        "        # Update per-class metrics if class names are provided\n",
        "        if class_names is not None:\n",
        "            update_per_class_metrics(history, val_metrics['all_labels'], val_metrics['all_preds'])\n",
        "\n",
        "            # Generate confusion matrix for this epoch\n",
        "            plot_confusion_matrix(val_metrics['all_labels'], val_metrics['all_preds'], class_names, epoch, save_path)\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print_epoch_metrics(epoch, epochs, train_loss, val_metrics)\n",
        "\n",
        "        # Early stopping and model saving logic\n",
        "        counter = handle_early_stopping(model, tokenizer, val_metrics['avg_val_loss'],\n",
        "                                      best_val_loss, counter, patience,\n",
        "                                      save_path, val_metrics['all_labels'],\n",
        "                                      val_metrics['all_preds'])\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered setelah {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Update best validation loss if improved\n",
        "        if val_metrics['avg_val_loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['avg_val_loss']\n",
        "\n",
        "    print(f\"Pelatihan selesai! Model terbaik disimpan di {save_path}\")\n",
        "\n",
        "    # Generate enhanced visualizations\n",
        "    enhanced_plot_training_results(history, save_path, class_names=class_names)\n",
        "\n",
        "    # Simpan history ke file JSON\n",
        "    save_enhanced_history(history, save_path)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def initialize_training_history():\n",
        "    \"\"\"Initialize the training history dictionary with all required keys\"\"\"\n",
        "    return {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_f1': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'batch_metrics': {\n",
        "            'iteration': [],\n",
        "            'loss': [],\n",
        "            'epoch': [],\n",
        "            'progress': [],\n",
        "            'learning_rates': []\n",
        "        },\n",
        "        'class_f1': [],  # Per-class F1 scores\n",
        "        'class_precision': [],  # Per-class precision\n",
        "        'class_recall': []  # Per-class recall\n",
        "    }\n",
        "\n",
        "def run_training_epoch(model, device, train_dataloader, optimizer, scheduler, loss_fn, epoch, epochs, history):\n",
        "    \"\"\"Run a single training epoch and return average loss\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} - Training dimulai...\")\n",
        "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),\n",
        "                      desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        try:\n",
        "            # Pindahkan batch ke device\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass with custom loss function\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Track batch-level metrics\n",
        "            update_batch_metrics(history, batch_idx, loss.item(), epoch, len(train_dataloader), optimizer)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(\"Peringatan: Kehabisan memori! Membersihkan cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    return train_loss / len(train_dataloader)\n",
        "\n",
        "def update_batch_metrics(history, batch_idx, loss_item, epoch, iterations_per_epoch, optimizer):\n",
        "    \"\"\"Update history with batch-level metrics\"\"\"\n",
        "    global_iteration = epoch * iterations_per_epoch + batch_idx\n",
        "    progress = (epoch + (batch_idx / iterations_per_epoch)) * 100\n",
        "\n",
        "    history['batch_metrics']['iteration'].append(global_iteration)\n",
        "    history['batch_metrics']['loss'].append(loss_item)\n",
        "    history['batch_metrics']['epoch'].append(epoch)\n",
        "    history['batch_metrics']['progress'].append(progress)\n",
        "    history['batch_metrics']['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs):\n",
        "    \"\"\"Run a single validation epoch and return metrics\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Validasi dimulai...\")\n",
        "    progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'avg_val_loss': avg_val_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "def update_history_with_validation_metrics(history, val_metrics):\n",
        "    \"\"\"Update history with validation metrics\"\"\"\n",
        "    history['val_loss'].append(val_metrics['avg_val_loss'])\n",
        "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['val_recall'].append(val_metrics['recall'])\n",
        "\n",
        "def update_per_class_metrics(history, all_labels, all_preds):\n",
        "    \"\"\"Update per-class metrics in history\"\"\"\n",
        "    class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    history['class_f1'].append(class_f1.tolist())\n",
        "    history['class_precision'].append(class_precision.tolist())\n",
        "    history['class_recall'].append(class_recall.tolist())\n",
        "\n",
        "def print_epoch_metrics(epoch, epochs, avg_train_loss, val_metrics):\n",
        "    \"\"\"Print detailed metrics for an epoch\"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_metrics['avg_val_loss']:.4f}, Val Accuracy: {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Val F1: {val_metrics['f1']:.4f}, Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}\")\n",
        "    if 'all_labels' in val_metrics and 'all_preds' in val_metrics:\n",
        "        print(f\"\\nClass-wise precision/recall/F1 setelah epoch {epoch+1}:\")\n",
        "        print(classification_report(val_metrics['all_labels'], val_metrics['all_preds'], digits=4))\n",
        "def handle_early_stopping(model, tokenizer, avg_val_loss, best_val_loss, counter, patience, save_path, all_labels, all_preds):\n",
        "    \"\"\"Handle early stopping logic and model saving\"\"\"\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0  # Reset early stopping counter\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        print(f\"Menyimpan model terbaik ke {save_path}\")\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        # Save classification report for best model\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "        with open(os.path.join(save_path, \"classification_report.json\"), 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Validation loss tidak membaik. Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pWFt6WSazwgT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Evaluation\n",
        "def evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, save_path):\n",
        "    \"\"\"Enhanced model evaluation with better visualizations\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare dataset and dataloader\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluasi Model\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                 target_names=intent_classes,\n",
        "                                 output_dict=True)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df = report_df.round(3)\n",
        "\n",
        "    # Filter for class metrics only (exclude summary rows)\n",
        "    class_df = report_df.loc[intent_classes]\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    # Save classification metrics as CSV\n",
        "    report_df.to_csv(os.path.join(save_path, \"classification_report.csv\"))\n",
        "\n",
        "    # Create visualizations\n",
        "    create_static_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "    create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "\n",
        "    # Print report summary\n",
        "    print(\"\\nModel Evaluation Report:\")\n",
        "    print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "    print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Weighted F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "def create_static_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save static matplotlib visualizations\"\"\"\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=intent_classes, yticklabels=intent_classes)\n",
        "    plt.title('Confusion Matrix - Final Model')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"final_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Classification report heatmap\n",
        "    plt.figure(figsize=(12, len(intent_classes)*0.5 + 3))\n",
        "    sns.heatmap(class_df[metrics], annot=True, cmap='YlGnBu', fmt='.3f',\n",
        "               yticklabels=intent_classes, cbar=True)\n",
        "    plt.title('Performance Metrics by Intent Class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"class_performance_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save interactive plotly visualizations\"\"\"\n",
        "    # Interactive confusion matrix\n",
        "    fig_cm = px.imshow(cm,\n",
        "                   labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                   x=intent_classes, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='Blues')\n",
        "\n",
        "    fig_cm.update_layout(\n",
        "        title='Confusion Matrix (Interactive)',\n",
        "        width=900,\n",
        "        height=800\n",
        "    )\n",
        "    fig_cm.write_html(os.path.join(save_path, \"interactive_confusion_matrix.html\"))\n",
        "\n",
        "    # Interactive performance metrics\n",
        "    fig_perf = px.imshow(class_df[metrics],\n",
        "                   labels=dict(x=\"Metric\", y=\"Intent Class\", color=\"Score\"),\n",
        "                   x=metrics, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='YlGnBu')\n",
        "\n",
        "    fig_perf.update_layout(\n",
        "        title='Performance Metrics by Intent Class (Interactive)',\n",
        "        width=800,\n",
        "        height=max(400, len(intent_classes)*30)\n",
        "    )\n",
        "    fig_perf.write_html(os.path.join(save_path, \"interactive_class_performance.html\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "def enhanced_plot_training_results(history, save_path, class_names=None):\n",
        "    \"\"\"\n",
        "    Enhanced function to plot training results with more detailed visualizations\n",
        "\n",
        "    Args:\n",
        "        history: Dictionary containing training history metrics\n",
        "        save_path: Path to save visualization files\n",
        "        class_names: Optional list of class names for confusion matrix\n",
        "    \"\"\"\n",
        "    # Create the static plots (same as before for compatibility)\n",
        "    static_plot_training_results(history, save_path)\n",
        "\n",
        "    # Create interactive plotly visualizations\n",
        "    interactive_plot_training_results(history, save_path)\n",
        "\n",
        "    # If we have class metrics in our history, plot those too\n",
        "    if 'class_f1' in history and class_names is not None:\n",
        "        plot_class_metrics(history, save_path, class_names)\n",
        "\n",
        "    # If we tracked learning rates, plot those\n",
        "    if 'batch_metrics' in history and 'learning_rates' in history['batch_metrics']:\n",
        "        plot_learning_rate(history, save_path)\n",
        "\n",
        "def static_plot_training_results(history, save_path):\n",
        "    \"\"\"Plot and save training metrics using matplotlib (static)\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Loss selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy', marker='o', color='green')\n",
        "    plt.title('Akurasi selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: F1 Score\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history['val_f1'], label='Validation F1', marker='o', color='purple')\n",
        "    plt.title('F1 Score selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Precision & Recall\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history['val_precision'], label='Validation Precision', marker='o', color='orange')\n",
        "    plt.plot(history['val_recall'], label='Validation Recall', marker='o', color='brown')\n",
        "    plt.title('Precision & Recall selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(os.path.join(save_path, \"training_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def interactive_plot_training_results(history, save_path):\n",
        "    \"\"\"Create interactive plotly visualizations of training metrics\"\"\"\n",
        "    # Create epochs list for x-axis\n",
        "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Loss': history['train_loss'],\n",
        "        'Validation Loss': history['val_loss'],\n",
        "        'Validation Accuracy': history['val_accuracy'],\n",
        "        'Validation F1': history['val_f1'],\n",
        "        'Validation Precision': history['val_precision'],\n",
        "        'Validation Recall': history['val_recall']\n",
        "    })\n",
        "\n",
        "    # Create subplot figure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Loss', 'Accuracy', 'F1 Score', 'Precision & Recall'),\n",
        "        vertical_spacing=0.15,\n",
        "        horizontal_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # Add traces for each metric\n",
        "    # Loss plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['train_loss'], mode='lines+markers', name='Training Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_loss'], mode='lines+markers', name='Validation Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Accuracy plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_accuracy'], mode='lines+markers', name='Validation Accuracy', line=dict(color='green')),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # F1 Score plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_f1'], mode='lines+markers', name='Validation F1', line=dict(color='purple')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Precision & Recall plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_precision'], mode='lines+markers', name='Validation Precision', line=dict(color='orange')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_recall'], mode='lines+markers', name='Validation Recall', line=dict(color='brown')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        width=1200,\n",
        "        title_text=\"Training Metrics (Interactive)\",\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "\n",
        "    # Save interactive plot as HTML\n",
        "    fig.write_html(os.path.join(save_path, \"interactive_training_metrics.html\"))\n",
        "\n",
        "    # Create a combined metrics plot for better trend comparison\n",
        "    fig_combined = px.line(\n",
        "        df,\n",
        "        x='Epoch',\n",
        "        y=['Training Loss', 'Validation Loss', 'Validation Accuracy', 'Validation F1', 'Validation Precision', 'Validation Recall'],\n",
        "        title='All Training Metrics',\n",
        "        labels={'value': 'Metric Value', 'variable': 'Metric'}\n",
        "    )\n",
        "\n",
        "    fig_combined.update_layout(height=600, width=1000, hovermode=\"x unified\")\n",
        "    fig_combined.write_html(os.path.join(save_path, \"combined_metrics.html\"))\n",
        "\n",
        "def plot_confusion_matrix(all_labels, all_preds, class_names, epoch, save_path):\n",
        "    \"\"\"Plot and save confusion matrix for the epoch\"\"\"\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the confusion matrix\n",
        "    cm_dir = os.path.join(save_path, \"confusion_matrices\")\n",
        "    os.makedirs(cm_dir, exist_ok=True)\n",
        "    plt.savefig(os.path.join(cm_dir, f\"cm_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_class_metrics(history, save_path, class_names):\n",
        "    \"\"\"Plot per-class performance metrics\"\"\"\n",
        "    # Create directory for class metrics\n",
        "    os.makedirs(os.path.join(save_path, \"class_metrics\"), exist_ok=True)\n",
        "\n",
        "    # Plot F1 per class if available\n",
        "    if 'class_f1' in history:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Convert dictionary structure to usable format\n",
        "        epochs = len(history['class_f1'])\n",
        "        x_epochs = list(range(1, epochs + 1))\n",
        "\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            plt.plot(x_epochs, class_f1, marker='o', label=f'{class_name}')\n",
        "\n",
        "        plt.title('F1 Score per Class')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.grid(True)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, \"class_metrics\", \"f1_per_class.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Interactive version with plotly\n",
        "        fig = go.Figure()\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=x_epochs,\n",
        "                y=class_f1,\n",
        "                mode='lines+markers',\n",
        "                name=class_name\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='F1 Score per Class (Interactive)',\n",
        "            xaxis_title='Epoch',\n",
        "            yaxis_title='F1 Score',\n",
        "            height=600,\n",
        "            width=1000,\n",
        "            hovermode=\"x unified\"\n",
        "        )\n",
        "        fig.write_html(os.path.join(save_path, \"class_metrics\", \"f1_per_class.html\"))\n",
        "\n",
        "def plot_learning_rate(history, save_path):\n",
        "    \"\"\"Plot learning rate changes over training\"\"\"\n",
        "    # Extract learning rates from batch metrics\n",
        "    iterations = history['batch_metrics']['iteration']\n",
        "    learning_rates = history['batch_metrics']['learning_rates']\n",
        "    epochs = history['batch_metrics']['epoch']\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(iterations, learning_rates)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"learning_rate_schedule.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Interactive version\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=iterations,\n",
        "        y=learning_rates,\n",
        "        mode='lines',\n",
        "        name='Learning Rate'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Learning Rate Schedule (Interactive)',\n",
        "        xaxis_title='Iteration',\n",
        "        yaxis_title='Learning Rate',\n",
        "        height=500,\n",
        "        width=900\n",
        "    )\n",
        "    fig.write_html(os.path.join(save_path, \"learning_rate_schedule.html\"))\n",
        "\n",
        "def save_enhanced_history(history, save_path):\n",
        "    \"\"\"Save enhanced training history with additional visualizations\"\"\"\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    processed_history = {}\n",
        "\n",
        "    for key, value in history.items():\n",
        "        if isinstance(value, dict):\n",
        "            processed_history[key] = {}\n",
        "            for subkey, subvalue in value.items():\n",
        "                processed_history[key][subkey] = convert_to_serializable(subvalue)\n",
        "        else:\n",
        "            processed_history[key] = convert_to_serializable(value)\n",
        "\n",
        "    # Save the enhanced history\n",
        "    with open(os.path.join(save_path, \"enhanced_training_history.json\"), 'w') as f:\n",
        "        json.dump(processed_history, f, indent=4)\n",
        "\n",
        "    print(f\"Enhanced training history saved to {os.path.join(save_path, 'enhanced_training_history.json')}\")\n",
        "\n",
        "def convert_to_serializable(value):\n",
        "    \"\"\"Convert numpy arrays and other non-serializable types to JSON-compatible types\"\"\"\n",
        "    if isinstance(value, np.ndarray):\n",
        "        return value.tolist()\n",
        "    elif isinstance(value, list):\n",
        "        if value and isinstance(value[0], np.ndarray):\n",
        "            return [item.tolist() if isinstance(item, np.ndarray) else item for item in value]\n",
        "        else:\n",
        "            return value\n",
        "    else:\n",
        "        return value"
      ],
      "metadata": {
        "id": "5y2_yA7p1I_E",
        "cellView": "form"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Upp9G0xn0fN7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PredictionIntent\n",
        "def predict_intent_with_enhanced_ood(text, model, tokenizer, intent_classes,\n",
        "                                     energy_threshold, msp_threshold=None,\n",
        "                                     temperature=1.0, method='combined',\n",
        "                                     label_encoder=None, device=None,\n",
        "                                     return_logits=False):\n",
        "    \"\"\"\n",
        "    Memprediksi intent dari teks input dengan deteksi Out-of-Distribution yang ditingkatkan\n",
        "\n",
        "    Args:\n",
        "        text: Teks input untuk diprediksi\n",
        "        model: Model yang sudah dilatih\n",
        "        tokenizer: Tokenizer untuk model\n",
        "        intent_classes: List nama intent\n",
        "        energy_threshold: Threshold untuk energy-based OOD detection\n",
        "        msp_threshold: Threshold untuk MSP-based OOD detection\n",
        "        temperature: Parameter temperature untuk energy\n",
        "        method: Metode deteksi OOD - 'energy', 'msp', atau 'combined'\n",
        "        label_encoder: Label encoder untuk intent classes\n",
        "        device: Device untuk inference\n",
        "        return_logits: Jika True, mengembalikan logits asli\n",
        "\n",
        "    Returns:\n",
        "        dict: Hasil prediksi dengan detail OOD detection\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = [text]  # Convert single text to list\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # List untuk menyimpan hasil setiap input\n",
        "    results = []\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Hitung energy score: -T*log(sum(exp(logits/T)))\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "\n",
        "        # Hitung confidence dengan softmax\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "        max_probs = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            prediction = predictions[i].item()\n",
        "            energy_score = energy[i].item()\n",
        "            confidence = max_probs[i].item()\n",
        "\n",
        "            # Deteksi OOD berdasarkan metode yang dipilih\n",
        "            is_ood_energy = energy_score > energy_threshold if energy_threshold is not None else False\n",
        "            is_ood_msp = confidence < msp_threshold if msp_threshold is not None else False\n",
        "\n",
        "            if method == 'energy':\n",
        "                is_ood = is_ood_energy\n",
        "            elif method == 'msp':\n",
        "                is_ood = is_ood_msp\n",
        "            else:  # 'combined'\n",
        "                is_ood = is_ood_energy and is_ood_msp\n",
        "\n",
        "            # Tentukan intent berdasarkan hasil OOD detection\n",
        "            if is_ood:\n",
        "                predicted_intent = \"unknown\"\n",
        "                topk_intents = [(\"unknown\", 1.0)]  # Unknown intent dengan confidence 100%\n",
        "            else:\n",
        "                predicted_intent = intent_classes[prediction]\n",
        "\n",
        "                # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "                top_k = min(3, len(intent_classes))\n",
        "                topk_values, topk_indices = torch.topk(probabilities[i], top_k)\n",
        "                topk_intents = [(intent_classes[idx.item()], val.item())\n",
        "                                for idx, val in zip(topk_indices, topk_values)]\n",
        "\n",
        "            # Buat hasil untuk input ini\n",
        "            result = {\n",
        "                \"text\": text[i],\n",
        "                \"intent\": predicted_intent,\n",
        "                \"confidence\": confidence,\n",
        "                \"energy_score\": energy_score,\n",
        "                \"is_ood\": is_ood,\n",
        "                \"is_ood_energy\": is_ood_energy,\n",
        "                \"is_ood_msp\": is_ood_msp if msp_threshold is not None else None,\n",
        "                \"top_intents\": topk_intents\n",
        "            }\n",
        "\n",
        "            if return_logits:\n",
        "                result[\"logits\"] = logits[i].cpu().numpy()\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    # Jika hanya satu input, kembalikan hasil langsung tanpa list\n",
        "    if len(text) == 1:\n",
        "        return results[0]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fDr3k0Ir0rOU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run pipeline\n",
        "def run_full_pipeline_enhanced(\n",
        "    use_drive=True,\n",
        "    percentile=95,\n",
        "    ood_method='combined',\n",
        "    split_dataset=\"no\",\n",
        "    val_split=0.2,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    patience=3,\n",
        "    train_csv_path=\"train.csv\",\n",
        "    val_csv_path=\"val.csv\"\n",
        "):\n",
        "    \"\"\"Jalankan pipeline lengkap dengan enhanced OOD detection dan visualisasi yang ditingkatkan\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    use_drive : bool\n",
        "        Apakah menggunakan Google Drive untuk penyimpanan\n",
        "    percentile : int\n",
        "        Persentil untuk threshold OOD detection\n",
        "    ood_method : str\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    split_dataset : str\n",
        "        Mode pemisahan dataset (\"yes\" untuk split dari train.csv, \"no\" untuk file terpisah)\n",
        "    val_split : float\n",
        "        Proporsi data validasi jika split_dataset=\"yes\"\n",
        "    batch_size : int\n",
        "        Ukuran batch untuk training dan evaluasi\n",
        "    epochs : int\n",
        "        Jumlah epoch training\n",
        "    learning_rate : float\n",
        "        Learning rate optimizer\n",
        "    weight_decay : float\n",
        "        Weight decay untuk regularisasi\n",
        "    patience : int\n",
        "        Early stopping patience\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder = prepare_data(\n",
        "        split_dataset, val_split, train_csv_path, val_csv_path)\n",
        "\n",
        "    num_labels = len(intent_classes)\n",
        "\n",
        "    model, tokenizer = setup_indobert_for_intent(num_labels)\n",
        "\n",
        "    model, history = train_intent_classifier(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_texts,\n",
        "        train_labels,\n",
        "        val_texts,\n",
        "        val_labels,\n",
        "        class_names=intent_classes,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        patience=patience\n",
        "    )\n",
        "\n",
        "    thresholds = calibrate_and_evaluate(model, tokenizer, val_texts, val_labels,\n",
        "                                       intent_classes, percentile)\n",
        "\n",
        "    save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history)\n",
        "\n",
        "    print_summary(num_labels, intent_classes, thresholds)\n",
        "\n",
        "    run_prediction_demo_enhanced(model, tokenizer, intent_classes, label_encoder, method=ood_method)\n",
        "\n",
        "    return model, tokenizer, intent_classes, label_encoder\n",
        "\n",
        "\n",
        "def prepare_data(split_dataset, val_split, train_csv_path=\"train.csv\", val_csv_path=\"val.csv\"):\n",
        "    \"\"\"Prepare training and validation data based on split mode\"\"\"\n",
        "\n",
        "    if split_dataset.lower() == \"yes\":\n",
        "        all_texts, all_labels, intent_classes, label_encoder = load_csv_data(train_csv_path, show_distribution=True)\n",
        "\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "            all_texts, all_labels, test_size=val_split, random_state=42, stratify=all_labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\n Dataset telah dibagi: {len(train_texts)} data training dan {len(val_texts)} data validasi\")\n",
        "    else:\n",
        "        train_texts, train_labels, intent_classes, label_encoder = load_csv_data(train_csv_path, show_distribution=True)\n",
        "        val_texts, val_labels, _, _ = load_csv_data(val_csv_path, label_encoder=label_encoder, show_distribution=True)\n",
        "\n",
        "    return train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder\n",
        "\n",
        "\n",
        "def calibrate_and_evaluate(model, tokenizer, val_texts, val_labels, intent_classes, percentile):\n",
        "    \"\"\"Calibrate OOD detection and evaluate model\"\"\"\n",
        "    # Prepare validation dataloader for OOD calibration\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # Calibrate OOD detection\n",
        "    thresholds = enhanced_calibrate_ood(model, tokenizer, val_dataloader, MODEL_SAVE_PATH, percentile=percentile)\n",
        "\n",
        "    # Evaluate model and generate visualizations\n",
        "    report, cm = evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, MODEL_SAVE_PATH)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "\n",
        "def save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history):\n",
        "    \"\"\"Save model artifacts and visualizations\"\"\"\n",
        "    # Generate enhanced visualizations for the final model\n",
        "    enhanced_plot_training_results(history, MODEL_SAVE_PATH, class_names=intent_classes)\n",
        "    save_enhanced_history(history, MODEL_SAVE_PATH)\n",
        "\n",
        "    # Save intent classes & label encoder\n",
        "    with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"wb\") as f:\n",
        "        pickle.dump(intent_classes, f)\n",
        "\n",
        "    with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"\\n Model telah berhasil dilatih dan disimpan di {MODEL_SAVE_PATH}\")\n",
        "\n",
        "def print_summary(num_labels, intent_classes, thresholds):\n",
        "    \"\"\"Print summary information about the trained model\"\"\"\n",
        "    print(f\"Jumlah intent: {num_labels}\")\n",
        "    print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "    print(f\"OOD detection thresholds: Energy={thresholds['energy_threshold']:.4f}, MSP={thresholds['msp_threshold']:.4f}\")\n",
        "    print(f\"Visualisasi training telah disimpan di {MODEL_SAVE_PATH}\")\n",
        "    print(f\"- Interactive plots dapat dibuka pada file HTML di folder tersebut\")\n",
        "    print(f\"- Static plots tersedia dalam format PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dq-WV98s0zgH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run Prediksi\n",
        "def run_prediction_demo_enhanced(model=None, tokenizer=None, intent_classes=None, label_encoder=None, model_path=None, method='combined', test_texts=None):\n",
        "    \"\"\"Jalankan demo prediksi intent dengan model yang telah dilatih dan enhanced OOD detection\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Model object, optional\n",
        "        Model yang sudah dilatih\n",
        "    tokenizer : Tokenizer object, optional\n",
        "        Tokenizer yang sesuai dengan model\n",
        "    intent_classes : list, optional\n",
        "        Daftar kelas intent\n",
        "    label_encoder : LabelEncoder, optional\n",
        "        Label encoder yang digunakan saat training\n",
        "    model_path : str, optional\n",
        "        Path ke model tersimpan (digunakan jika model=None)\n",
        "    method : str, optional\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    test_texts : list, optional\n",
        "        Daftar teks untuk diprediksi secara batch. Setelah batch, akan lanjut ke mode interaktif.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = MODEL_SAVE_PATH\n",
        "\n",
        "    # Jika model tidak diberikan, muat dari path penyimpanan\n",
        "    if model is None or tokenizer is None or intent_classes is None:\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Error: Model tidak ditemukan di {model_path}\")\n",
        "            print(\"Jalankan run_full_pipeline() terlebih dahulu untuk melatih model\")\n",
        "            return\n",
        "\n",
        "        # Muat model dan tokenizer\n",
        "        print(f\"Memuat model dari {model_path}...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Muat intent classes\n",
        "        import pickle\n",
        "        with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "            intent_classes = pickle.load(f)\n",
        "            print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "\n",
        "    # Load OOD thresholds\n",
        "    thresholds = load_ood_thresholds(model_path)\n",
        "    energy_threshold = thresholds[\"energy_threshold\"]\n",
        "    msp_threshold = thresholds.get(\"msp_threshold\")\n",
        "\n",
        "    if msp_threshold is not None:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP={msp_threshold:.4f}\")\n",
        "    else:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP=None\")\n",
        "\n",
        "    print(f\"Menggunakan metode deteksi OOD: {method}\")\n",
        "\n",
        "    print(\"\\nDemo Prediksi Intent dengan Enhanced OOD Detection:\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Helper function untuk memprediksi dan menampilkan hasil\n",
        "    def predict_and_display(text):\n",
        "        result = predict_intent_with_enhanced_ood(\n",
        "            text,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            intent_classes,\n",
        "            energy_threshold,\n",
        "            msp_threshold,\n",
        "            method=method\n",
        "        )\n",
        "\n",
        "        if result[\"is_ood\"]:\n",
        "            print(f\" Intent terdeteksi: unknown\")\n",
        "            print(f\"   Energy score: {result['energy_score']:.4f} (threshold: {energy_threshold:.4f})\")\n",
        "            if msp_threshold:\n",
        "                print(f\"   Confidence score: {result['confidence']:.4f} (threshold: {msp_threshold:.4f})\")\n",
        "        else:\n",
        "            print(f\" Intent terdeteksi: {result['intent']} (confidence: {result['confidence']:.4f})\")\n",
        "\n",
        "        print(\"\\nTop 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(result[\"top_intents\"]):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n",
        "        print(\"\\nDetail OOD detection:\")\n",
        "        print(f\"  Energy-based: {'OOD' if result['is_ood_energy'] else 'In-Distribution'} ({result['energy_score']:.4f})\")\n",
        "        if msp_threshold:\n",
        "            print(f\"  MSP-based: {'OOD' if result['is_ood_msp'] else 'In-Distribution'} ({result['confidence']:.4f})\")\n",
        "        print(f\"  Final decision: {'OOD' if result['is_ood'] else 'In-Distribution'}\")\n",
        "\n",
        "    # Jika test_texts diberikan, lakukan prediksi batch\n",
        "    if test_texts is not None and isinstance(test_texts, list) and len(test_texts) > 0:\n",
        "        print(f\"\\nMemprediksi {len(test_texts)} contoh teks:\")\n",
        "        print(\"----------------------------\")\n",
        "\n",
        "        for i, text in enumerate(test_texts):\n",
        "            print(f\"\\nContoh #{i+1}: \\\"{text}\\\"\")\n",
        "            predict_and_display(text)\n",
        "\n",
        "        print(\"\\n----------------------------\")\n",
        "        print(\"Selesai memprediksi contoh teks. Beralih ke mode interaktif.\")\n",
        "\n",
        "    # Mode interaktif\n",
        "    print(\"\\nMode Interaktif - Masukkan teks untuk prediksi intent\")\n",
        "    print(\"Ketik 'exit' untuk keluar\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    # Prediksi input pengguna\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        predict_and_display(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpUFrU9D-qPt"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Jalankan pipeline (uncomment untuk menjalankan)\n",
        "\n",
        "model, tokenizer, intent_classes, label_encoder = run_full_pipeline_enhanced(\n",
        "    use_drive=True,\n",
        "    percentile=90,\n",
        "    ood_method='combined',\n",
        "    split_dataset=\"yes\",\n",
        "    val_split=0.20,\n",
        "    batch_size=32,\n",
        "    epochs=12,\n",
        "    learning_rate=2.5e-5,\n",
        "    weight_decay=0.01,\n",
        "    patience=3,\n",
        "    train_csv_path=\"train.csv\",\n",
        "    val_csv_path=\"val.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Set\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# ==== Step 1: Load test data ====\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_texts = test_df['text'].tolist()\n",
        "y_true = test_df['intent'].tolist()  # Assuming the true labels column is named 'intent'\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "# ==== Step 2: Load label encoder and intent_classes ====\n",
        "with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"rb\") as f: # Load intent_classes\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Encode the ground truth labels into numerical format\n",
        "y_true_encoded = label_encoder.transform(y_true)\n",
        "\n",
        "# ==== Step 3: Define prediction function ====\n",
        "def get_model_predictions(texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():  # Important to avoid unnecessary gradient calculations\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits.softmax(dim=1)\n",
        "            pred = probs.argmax(dim=1).item()\n",
        "            predictions.append(pred)\n",
        "    return predictions\n",
        "\n",
        "# ==== Step 4: Get predictions ====\n",
        "y_pred = get_model_predictions(test_texts)\n",
        "y_pred_labels = [intent_classes[i] for i in y_pred]\n",
        "\n",
        "# ==== Step 5: Print Classification Report ====\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred_labels, target_names=intent_classes))\n",
        "\n",
        "# ==== Step 6: Confusion Matrix (encoded labels for alignment) ====\n",
        "cm = confusion_matrix(y_true_encoded, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=intent_classes)\n",
        "\n",
        "# ==== Step 7: Plot Confusion Matrix ====\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation='vertical', ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_normalized, annot=True, cmap=\"Blues\", xticklabels=intent_classes, yticklabels=intent_classes, fmt=\".2f\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a list to store misclassified examples\n",
        "misclassified_examples = []\n",
        "\n",
        "for i, (true, pred_idx, text) in enumerate(zip(y_true_encoded, y_pred, test_texts)):\n",
        "    if true != pred_idx:\n",
        "        misclassified_examples.append({\n",
        "            'text': text,\n",
        "            'true_label': intent_classes[true],\n",
        "            'predicted_label': intent_classes[pred_idx]\n",
        "        })\n",
        "\n",
        "# Save misclassified examples to CSV\n",
        "if misclassified_examples:\n",
        "    misclassified_df = pd.DataFrame(misclassified_examples)\n",
        "    misclassified_csv_path = f\"{MODEL_SAVE_PATH}/misclassified_examples.csv\"\n",
        "    misclassified_df.to_csv(misclassified_csv_path, index=False)\n",
        "    print(f\"Saved {len(misclassified_examples)} misclassified examples to {misclassified_csv_path}\")\n",
        "else:\n",
        "    print(\"No misclassified examples found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLOKcYqVT-yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "test_sentences = [\n",
        "    # GREETING\n",
        "    \"Halo, selamat pagi!\",\n",
        "    \"Apa kabar?\",\n",
        "    \"Hai, bot!\",\n",
        "    \"Permisi, boleh bertanya?\",\n",
        "    \"Yo, ada orang di sana?\",\n",
        "\n",
        "    # GOODBYE\n",
        "    \"Terima kasih, sampai jumpa.\",\n",
        "    \"Ok, saya pergi dulu.\",\n",
        "    \"Sampai nanti!\",\n",
        "    \"Dadah, bot.\",\n",
        "    \"Aku akan kembali nanti.\",\n",
        "\n",
        "    # CONFIRM\n",
        "    \"Iya, benar.\",\n",
        "    \"Betul sekali.\",\n",
        "    \"Ya, saya setuju.\",\n",
        "    \"Tentu saja.\",\n",
        "    \"Itu yang saya maksud.\",\n",
        "\n",
        "    # DENIED\n",
        "    \"Tidak, bukan itu.\",\n",
        "    \"Salah.\",\n",
        "    \"Bukan, maksud saya yang lain.\",\n",
        "    \"Enggak.\",\n",
        "    \"Saya tidak yakin dengan itu.\",\n",
        "\n",
        "    # AMBIGUOUS (bisa mengecoh)\n",
        "    \"Saya rasa tidak perlu, tapi ya juga boleh.\",\n",
        "    \"Mungkin... tapi entahlah.\",\n",
        "    \"Terserah kamu aja deh.\",\n",
        "    \"Boleh iya, boleh juga tidak.\",\n",
        "    \"Ya tapi tidak juga sih...\",\n",
        "    \"p\",\n",
        "    \"test\",\n",
        "    \"y\",\n",
        "    \"g\",\n",
        "    \"N\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "P9gOO73x_oxo",
        "cellView": "form"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_b2pfqxA8Z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Teks judul default\n",
        "# If you want to load an existing model and run predictions\n",
        "run_prediction_demo_enhanced( #model, tokenizer, intent_classes, label_encoder, method=ood_method\n",
        "    model_path=MODEL_SAVE_PATH,  # Your MODEL_SAVE_PATH\n",
        "    method='combined',  # Which OOD detection method to use\n",
        "    test_texts=test_sentences\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66tjkPIApbrL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Augmentation for Indonesian NLP\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download WordNet data (if not already downloaded)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# =========[ KONFIGURASI ]=========\n",
        "INPUT_FILE = \"/content/test.csv\"  # @param {type:\"string\"} Bisa CSV atau XLSX\n",
        "DATA_TYPE = \"test\" # @param [\"train\", \"val\", \"test\"]\n",
        "TARGET_SAMPLES_PER_CLASS = 100    # @param {type:\"integer\"} Target jumlah sampel per kelas\n",
        "USE_PARAPHRASE_MODEL = True       # @param {type:\"boolean\"} Aktifkan atau matikan paraphrase\n",
        "USE_BACK_TRANSLATION = False       # @param {type:\"boolean\"} Aktifkan atau matikan back-translation\n",
        "MIN_AUGMENTATIONS_PER_SAMPLE = 0  # @param {type:\"integer\"} Minimum augmentasi per sampel asli\n",
        "MAX_AUGMENTATIONS_PER_SAMPLE = 10  # @param {type:\"integer\"} Maximum augmentasi per sampel asli\n",
        "BATCH_SIZE = 16                   # @param {type:\"integer\"} Untuk batch processing\n",
        "\n",
        "# =========[ IMPROVED SYNONYM DICTIONARY ]=========\n",
        "# Kamus sinonim Indonesia untuk kata-kata umum\n",
        "id_synonyms = {\n",
        "    # Kata-kata terkait sapaan\n",
        "    \"saya\": [\"aku\", \"gue\", \"gua\", \"hamba\", \"beta\"],\n",
        "    \"kamu\": [\"anda\", \"engkau\", \"kalian\", \"elu\", \"dikau\"],\n",
        "    \"hai\": [\"halo\", \"hello\", \"hi\", \"hey\", \"hola\"],\n",
        "    \"selamat\": [\"slamat\", \"met\", \"congratz\"],\n",
        "    \"pagi\": [\"morning\", \"subuh\"],\n",
        "    \"siang\": [\"dzuhur\", \"sore\"],\n",
        "    \"malam\": [\"malem\", \"night\"],\n",
        "\n",
        "    # Kata-kata umum\n",
        "    \"tidak\": [\"tak\", \"tiada\", \"bukan\", \"enggak\", \"nggak\", \"gak\", \"kagak\"],\n",
        "    \"iya\": [\"ya\", \"yoi\", \"yup\", \"yo\", \"oke\", \"ok\", \"betul\"],\n",
        "    \"bagaimana\": [\"gimana\", \"bagaimane\", \"gmn\", \"bgmn\"],\n",
        "    \"mengapa\": [\"kenapa\", \"knp\", \"knapa\", \"ngapa\"],\n",
        "    \"karena\": [\"sebab\", \"lantaran\", \"gara-gara\"],\n",
        "    \"sangat\": [\"banget\", \"amat\", \"sekali\", \"sungguh\"],\n",
        "    \"bisa\": [\"dapat\", \"mampu\", \"sanggup\"],\n",
        "    \"mungkin\": [\"barangkali\", \"kali\", \"kayaknya\", \"kayanya\"],\n",
        "    \"terima kasih\": [\"makasih\", \"tks\", \"thx\", \"thanks\", \"tengkyu\", \"tengkiu\"],\n",
        "    \"tolong\": [\"bantu\", \"bantuin\", \"tlg\", \"help\"],\n",
        "    \"mau\": [\"ingin\", \"pengen\", \"berkenan\"],\n",
        "    \"pergi\": [\"berangkat\", \"cabut\", \"start\", \"jalan\"],\n",
        "    \"lihat\": [\"tengok\", \"liat\", \"ngeliat\", \"mantau\"],\n",
        "    \"cari\": [\"mencari\", \"nyari\", \"telusuri\"]\n",
        "}\n",
        "\n",
        "# =========[ INTENT-SPECIFIC SLANG DICTIONARIES ]=========\n",
        "# Kata gaul umum untuk semua intent\n",
        "common_slang = {\n",
        "    'tidak': 'gak',\n",
        "    'iya': 'yoi',\n",
        "    'terima kasih': 'makasih',\n",
        "    'saya': 'gw',\n",
        "    'kamu': 'lo',\n",
        "    'sedang': 'lagi',\n",
        "    'bagaimana': 'gimana',\n",
        "    'begitu': 'gitu',\n",
        "    'bisa': 'bsa',\n",
        "    'akan': 'bakal',\n",
        "    'untuk': 'buat',\n",
        "    'tahu': 'tau',\n",
        "    'apakah': 'apa',\n",
        "    'mengapa': 'kenapa',\n",
        "    'selamat': 'selamet',\n",
        "    'dengan': 'dgn',\n",
        "    'sangat': 'banget',\n",
        "    'nigga': 'nigger',\n",
        "    'coy': 'coek'\n",
        "}\n",
        "\n",
        "# Kamus kata gaul khusus intent\n",
        "intent_slang = {\n",
        "    'jam_layanan': {\n",
        "        'perpustakaan': 'perpus',\n",
        "        'buka': 'open',\n",
        "        'tutup': 'close',\n",
        "        'sampai': 'sampe',\n",
        "        'jam': 'jm',\n",
        "        'informasi': 'info',\n",
        "        'hari ini': 'hr ini',\n",
        "        'kapan': 'kpn',\n",
        "        'jadwal': 'jdwl',\n",
        "        'operasional': 'ops',\n",
        "        'layanan': 'lyn',\n",
        "        'masih': 'msih',\n",
        "        'minggu': 'mg',\n",
        "        'hari': 'hri',\n",
        "        'pukul': 'pkl',\n",
        "    },\n",
        "    'cari_buku': {\n",
        "        'mencari': 'nyari',\n",
        "        'mau mencari': 'mau nyari',\n",
        "        'ingin mencari': 'pengen nyari',\n",
        "        'tolong carikan': 'cariin',\n",
        "        'tolong bantu cari': 'bantuin cari',\n",
        "        'mencarikan': 'cariin',\n",
        "        'buku': 'book',\n",
        "        'butuh': 'need',\n",
        "        'melihat': 'liat',\n",
        "        'daftar': 'list',\n",
        "        'akses': 'akses',\n",
        "        'temukan': 'nemu',\n",
        "        'mencoba': 'nyoba',\n",
        "        'mengakses': 'akses',\n",
        "        'gunakan': 'make use',\n",
        "        'fitur': 'fitr',\n",
        "        'pencarian': 'search',\n",
        "        'referensi': 'ref',\n",
        "        'bantuan': 'bntuan',\n",
        "        'dimana': 'dmn',\n",
        "        'cek': 'check',\n",
        "        'lihat-lihat': 'liat2',\n",
        "        'cari': 'search',\n",
        "        'mencari buku': 'nyari book',\n",
        "    },\n",
        "    'greeting': {\n",
        "        'halo': 'haloo',\n",
        "        'hai': 'hay',\n",
        "        'hello': 'helo',\n",
        "        'selamat pagi': 'slmt pagi',\n",
        "        'selamat siang': 'slmt siang',\n",
        "        'selamat sore': 'slmt sore',\n",
        "        'selamat malam': 'slmt malam',\n",
        "        'apa kabar': 'apa kbr',\n",
        "        'assalamualaikum': 'asswrwb',\n",
        "        'permisi': 'permizz',\n",
        "        'hai bot': 'hey bot',\n",
        "        'bot': 'bt',\n",
        "        'selamat datang': 'slmt dtg',\n",
        "        # Additional variations for greeting\n",
        "        'halo selamat pagi': 'hai morning',\n",
        "        'hai selamat siang': 'helo siang',\n",
        "        'met pagi': 'morning',\n",
        "        'pagi': 'pgi',\n",
        "        'siang': 'siang boss',\n",
        "    },\n",
        "    'goodbye': {\n",
        "        'terima kasih': 'makasih',\n",
        "        'goodbye': 'gudbai',\n",
        "        'makasih': 'mksh',\n",
        "        'makasih ya': 'thx ya',\n",
        "        'sampai jumpa': 'sampe jmpa',\n",
        "        'dadah': 'daah',\n",
        "        'bye': 'byee',\n",
        "        'sampai nanti': 'sampe ntar',\n",
        "        'see you': 'cu',\n",
        "        'thanks': 'thx',\n",
        "        'thank you': 'tq',\n",
        "        'sekian': 'skian',\n",
        "        'itu saja': 'itu aj',\n",
        "        # Additional variations for goodbye\n",
        "        'ok makasih': 'ok thx',\n",
        "        'terima kasih banyak': 'thanks banget',\n",
        "        'makasih atas bantuannya': 'thx for helping',\n",
        "        'sampai bertemu lagi': 'see u later',\n",
        "        'selamat tinggal': 'bye bye',\n",
        "    },\n",
        "    'confirm': {\n",
        "        'betul': 'btl',\n",
        "        'setuju': 'stju',\n",
        "        'bener': 'bnr',\n",
        "        'iya benar': 'ya bnr',\n",
        "        'okey': 'okeyy',\n",
        "        'ok deh': 'okedeh',\n",
        "        # Additional variations for confirm\n",
        "        'tentu saja': 'tentu',\n",
        "        'saya setuju': 'aku setuju',\n",
        "        'benar sekali': 'bener banget',\n",
        "        'ya betul': 'yup betul',\n",
        "        'tentu boleh': 'boleh dong',\n",
        "        'setuju sekali': 'sangat setuju',\n",
        "    },\n",
        "    'denied': {\n",
        "        'tidak mau': 'gak mau',\n",
        "        'ga mau': 'gk mw',\n",
        "        'tidak setuju': 'gak setuju',\n",
        "        'saya tidak': 'aku ga',\n",
        "        'nggak perlu': 'ga perlu',\n",
        "        'ga usah': 'rasah',\n",
        "        'tidak perlu': 'gak usah',\n",
        "        'no': 'nope',\n",
        "        # Additional variations for denied\n",
        "        'saya tidak setuju': 'aku gak setuju',\n",
        "        'tidak bisa': 'ga bisa',\n",
        "        'tidak boleh': 'gak boleh',\n",
        "        'jangan': 'jgn',\n",
        "        'maaf tidak': 'sorry no',\n",
        "        'tidak begitu': 'gak gitu',\n",
        "    }\n",
        "}\n",
        "\n",
        "# =========[ PHONETIC AUGMENTATION DICTIONARY ]=========\n",
        "phonetic_dict = {\n",
        "        # Greeting related\n",
        "        \"saya\": [\"sy\", \"saia\", \"ane\", \"ana\", \"w\", \"gw\", \"q\", \"aq\"],\n",
        "        \"kamu\": [\"km\", \"kamyu\", \"u\", \"lo\", \"lu\", \"l\", \"ngana\", \"sampeyan\", \"antum\", \"ente\"],\n",
        "        \"halo\": [\"hlo\", \"hallo\", \"helo\", \"haloo\", \"hellow\", \"hy\", \"hyy\", \"p\", \"ping\"],\n",
        "        \"selamat\": [\"slmt\", \"slamat\", \"met\", \"slam\"],\n",
        "        \"pagi\": [\"pgi\", \"morning\", \"pg\", \"subuh\"],\n",
        "        \"siang\": [\"siang\", \"afternoon\", \"siank\", \"siyang\"],\n",
        "        \"malam\": [\"mlm\", \"malem\", \"mlem\", \"night\", \"evening\", \"mlem\", \"mlm\"],\n",
        "        \"apa\": [\"ap\", \"ape\", \"apah\", \"pa\"],\n",
        "        \"kabar\": [\"kbr\", \"kabare\", \"kbar\", \"kabbar\"],\n",
        "        \"gimana\": [\"gmn\", \"bgmn\", \"gmana\", \"gimane\", \"gmn\"],\n",
        "\n",
        "        # Goodbye related\n",
        "        \"sampai\": [\"smp\", \"sampe\", \"smpe\", \"smpei\", \"sampeyan\"],\n",
        "        \"jumpa\": [\"jpa\", \"jmpa\", \"ktmu\", \"jumpe\"],\n",
        "        \"dadah\": [\"byebye\", \"bye\", \"bay\", \"byee\", \"bbye\", \"bye2\", \"dadah\"],\n",
        "        \"pamit\": [\"pmt\", \"pamitt\", \"off\", \"out\", \"cabut\", \"cbut\"],\n",
        "        \"tinggal\": [\"tgl\", \"tnggal\", \"tinggel\", \"tggal\", \"tinggelin\"],\n",
        "        \"duluan\": [\"dlu\", \"duluan\", \"dluan\", \"dluan ya\", \"ahead\"],\n",
        "        \"pergi\": [\"pgi\", \"pegi\", \"prgi\", \"out\", \"keluar\"],\n",
        "        \"pulang\": [\"plg\", \"plng\", \"balik\", \"blk\", \"mudik\"],\n",
        "\n",
        "        # Confirm related\n",
        "        \"ya\": [\"y\", \"yah\", \"iye\", \"yoi\", \"yups\", \"yes\", \"yess\", \"yesss\", \"okey\", \"okeh\", \"oks\"],\n",
        "        \"setuju\": [\"stju\", \"acc\", \"accept\", \"approved\", \"approve\", \"deal\", \"oke\", \"ok\", \"sip\"],\n",
        "        \"benar\": [\"bnr\", \"bner\", \"bener\", \"bnr\", \"yoi\", \"correct\"],\n",
        "        \"sudah\": [\"sdh\", \"dah\", \"udh\", \"done\", \"wes\", \"uwes\", \"udah\", \"sdah\"],\n",
        "        \"bisa\": [\"bs\", \"bsa\", \"biza\", \"bsa\", \"biza\", \"ok\"],\n",
        "        \"pasti\": [\"pst\", \"psti\", \"pastii\", \"sure\", \"certain\"],\n",
        "        \"siap\": [\"sp\", \"ready\", \"sip\", \"sp\", \"roger\", \"on\", \"online\"],\n",
        "        \"jadi\": [\"jd\", \"jdi\", \"jdnya\", \"jdiin\", \"proceed\"],\n",
        "        \"lanjut\": [\"lnjt\", \"lanjt\", \"next\", \"go\"],\n",
        "        \"mantap\": [\"mntap\", \"mantab\", \"mntb\", \"top\", \"mantul\", \"josss\", \"kerennn\"],\n",
        "        \"bagus\": [\"bgs\", \"bgus\", \"nice\", \"naiss\", \"keren\", \"top\"],\n",
        "\n",
        "        # Denied related\n",
        "        \"tidak\": [\"tdk\", \"gak\", \"ga\", \"g\", \"nggak\", \"ngga\", \"nope\", \"no\", \"kagak\", \"kaga\", \"kgk\"],\n",
        "        \"jangan\": [\"jgn\", \"jngn\", \"don't\", \"dont\", \"jgn\", \"ga usah\", \"tdk usah\", \"gausa\", \"gausah\"],\n",
        "        \"belum\": [\"blm\", \"blom\", \"belom\", \"not yet\", \"durung\", \"durong\", \"belm\"],\n",
        "        \"batal\": [\"btl\", \"cancel\", \"cansel\", \"urungkan\", \"batalin\", \"gajadi\"],\n",
        "        \"maaf\": [\"sorry\", \"sori\", \"maf\", \"maap\", \"maaaaf\", \"mrff\", \"sry\", \"srry\"],\n",
        "        \"menolak\": [\"tlk\", \"reject\", \"decline\", \"dtolak\", \"nolak\", \"gak mau\", \"gamau\"],\n",
        "        \"mustahil\": [\"impossible\", \"ga mungkin\", \"g mungkin\", \"tdk mungkin\", \"gak bs\"],\n",
        "        \"salah\": [\"slh\", \"wrong\", \"error\", \"eror\", \"salh\", \"fail\"],\n",
        "        \"gagal\": [\"ggl\", \"fail\", \"failed\", \"error\", \"gagak\", \"failll\"],\n",
        "\n",
        "        # Common words\n",
        "        \"terima\": [\"trma\", \"thanks\", \"thx\", \"trims\", \"tq\", \"tyvm\", \"makasih\", \"mksih\"],\n",
        "        \"kasih\": [\"ksh\", \"ksih\", \"thx\", \"makasih\", \"mksih\", \"thanks\"],\n",
        "        \"tolong\": [\"tlng\", \"help\", \"tlg\", \"tulung\", \"bantu\", \"bantuin\"],\n",
        "        \"please\": [\"plz\", \"plis\", \"pliss\", \"plisss\", \"pliissss\", \"tolong\"],\n",
        "        \"besok\": [\"bsk\", \"bsok\", \"besok\", \"tmrw\", \"esok\", \"besuk\"],\n",
        "        \"waktu\": [\"wkt\", \"waktu\", \"time\", \"tm\", \"jam\"],\n",
        "        \"cukup\": [\"ckp\", \"enough\", \"cukuppp\", \"cukups\", \"ckup\"],\n",
        "        \"melihat\": [\"lihat\", \"liat\", \"look\", \"see\", \"watching\", \"ngeliat\"],\n",
        "        \"alasan\": [\"alsan\", \"reason\", \"why\", \"alesan\", \"alsn\"],\n",
        "        \"untuk\": [\"utk\", \"buat\", \"bwt\", \"4\", \"tuk\", \"2\", \"to\"],\n",
        "        \"melakukan\": [\"lakukan\", \"do\", \"lakuin\", \"melakuin\", \"ngerjain\"],\n",
        "        \"ini\": [\"ni\", \"this\", \"these\", \"iki\", \"nih\", \"ne\"],\n",
        "        \"kita\": [\"kta\", \"we\", \"us\", \"w\", \"kt\"],\n",
        "        \"hari\": [\"hr\", \"day\", \"hri\", \"days\", \"dayy\", \"harii\"],\n",
        "        \"bro\": [\"broh\", \"brow\", \"brother\", \"mas\", \"bang\", \"bor\", \"omm\"],\n",
        "        \"sis\": [\"sist\", \"sister\", \"mbak\", \"mba\", \"nte\", \"ceu\", \"teh\"]\n",
        "    }\n",
        "\n",
        "# =========[ READ & VALIDATE FILE ]=========\n",
        "def read_dataset(file_path):\n",
        "    \"\"\"Membaca dataset dari file CSV atau XLSX\"\"\"\n",
        "    print(f\"Loading dataset: {file_path}\")\n",
        "\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_ext == '.xlsx':\n",
        "        print(f\"Detected Excel file: {file_path}\")\n",
        "        df = pd.read_excel(file_path)\n",
        "        # Konversi ke CSV untuk kompatibilitas\n",
        "        csv_path = file_path.replace('.xlsx', '.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Converted Excel file to CSV: {csv_path}\")\n",
        "    elif file_ext == '.csv':\n",
        "        print(f\"Detected CSV file: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Format file tidak didukung: {file_ext}. Harap gunakan file CSV atau XLSX.\")\n",
        "\n",
        "    df = df.dropna()\n",
        "    print(f\"Dataset dimuat dengan {len(df)} baris\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========[ INITIALIZE PARAPHRASE MODEL IF NEEDED ]=========\n",
        "def initialize_paraphrase_model():\n",
        "    \"\"\"Initialize paraphrase model if enabled\"\"\"\n",
        "    if USE_PARAPHRASE_MODEL:\n",
        "        print(\"Loading paraphrase model...\")\n",
        "        start_time = time.time()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"Model loaded in {time.time() - start_time:.2f} seconds. Using device: {device}\")\n",
        "        return model, tokenizer\n",
        "    return None, None\n",
        "\n",
        "# =========[ AUGMENTATION METHODS ]=========\n",
        "def get_better_synonym(word):\n",
        "    \"\"\"Get synonym from custom dictionary or return the original word\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in id_synonyms:\n",
        "        synonyms = id_synonyms[word_lower]\n",
        "        return random.choice(synonyms)\n",
        "    return word\n",
        "\n",
        "def replace_with_synonym(sentence):\n",
        "    \"\"\"Replace words with synonyms while preserving capitalization\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        synonym = get_better_synonym(word)\n",
        "        # Preserve capitalization\n",
        "        if word and word[0].isupper() and synonym:\n",
        "            synonym = synonym[0].upper() + synonym[1:]\n",
        "        new_words.append(synonym)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def back_translate(sentence):\n",
        "    \"\"\"Translate to English and back to Indonesian\"\"\"\n",
        "    if not USE_BACK_TRANSLATION:\n",
        "        return sentence\n",
        "\n",
        "    try:\n",
        "        # First to English\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(sentence)\n",
        "        # Then back to Indonesian\n",
        "        back_translated = GoogleTranslator(source='en', target='id').translate(translated)\n",
        "\n",
        "        # Only return if result is different but not completely unrelated\n",
        "        if back_translated != sentence and len(back_translated.split()) >= len(sentence.split()) * 0.5:\n",
        "            return back_translated\n",
        "        return sentence\n",
        "    except Exception:\n",
        "        return sentence\n",
        "\n",
        "def add_typo(sentence):\n",
        "    \"\"\"Add a single typo by replacing a character\"\"\"\n",
        "    chars = list(sentence)\n",
        "    if len(chars) > 3:\n",
        "        idx = random.randint(0, len(chars) - 1)\n",
        "        if chars[idx].isalpha():  # Only replace letters\n",
        "            chars[idx] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def random_deletion(sentence, p=0.2):\n",
        "    \"\"\"Delete words with probability p\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) <= 3:  # Don't delete from very short sentences\n",
        "        return sentence\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p:\n",
        "            new_words.append(word)\n",
        "\n",
        "    # Make sure we don't delete everything\n",
        "    if not new_words:\n",
        "        return sentence\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    \"\"\"Swap n pairs of words\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "\n",
        "    for _ in range(min(n, len(words)//2)):  # Ensure we don't try too many swaps\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def phonetic_augmentation(sentence):\n",
        "    \"\"\"Apply phonetic substitutions common in Indonesian chat\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        if word_lower in phonetic_dict:\n",
        "            new_word = random.choice(phonetic_dict[word_lower])\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper():\n",
        "                new_word = new_word[0].upper() + new_word[1:]\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def apply_slang_typo(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply slang replacements with controllable intensity\"\"\"\n",
        "    # Combine common slang with intent-specific slang\n",
        "    slang_dict = common_slang.copy()\n",
        "    if intent in intent_slang:\n",
        "        slang_dict.update(intent_slang[intent])\n",
        "\n",
        "    # Create regex patterns from the slang dictionary\n",
        "    patterns = {\n",
        "        re.compile(rf'\\b{k}\\b', re.IGNORECASE): v for k, v in slang_dict.items()\n",
        "    }\n",
        "\n",
        "    # Apply only some of the patterns based on intensity\n",
        "    patterns_to_use = random.sample(\n",
        "        list(patterns.items()),\n",
        "        k=int(len(patterns) * min(1.0, intensity * 0.7))\n",
        "    )\n",
        "\n",
        "    for pattern, replacement in patterns_to_use:\n",
        "        text = pattern.sub(replacement, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def character_noise(text, intensity=1.0):\n",
        "    \"\"\"Add typos like character swaps, insertions, deletions\"\"\"\n",
        "    chars = list(text)\n",
        "    swap_prob = min(0.1, intensity * 0.05)  # Scale probability with intensity\n",
        "\n",
        "    # Character swaps\n",
        "    for i in range(len(chars)-1):\n",
        "        if random.random() < swap_prob:\n",
        "            chars[i], chars[i+1] = chars[i+1], chars[i]\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "def add_common_phrase(sentence):\n",
        "    \"\"\"Add a common Indonesian chat phrase\"\"\"\n",
        "    common_phrases = [\"sih\", \"ya\", \"dong\", \"cuy\", \"bro\", \"lah\", \"plis\", \"eh\",\n",
        "                    \"nih\", \"gitu\", \"kan\", \"yah\", \"deh\", \"banget\"]\n",
        "    return sentence + \" \" + random.choice(common_phrases)\n",
        "\n",
        "def short_text_augmentation(text, intent):\n",
        "    \"\"\"Special augmentation for very short texts like greetings and goodbyes\"\"\"\n",
        "    # For very short texts, add filler words or expressions\n",
        "    fillers = {\n",
        "        'greeting': ['', ' ya', ' kak', ' min', ' gan', ' bro', ' sis', ' admin', '!', '!!'],\n",
        "        'goodbye': ['', ' ya', ' kak', ' min', ' sekali lagi', ' semuanya', '!', '!!'],\n",
        "        'confirm': ['', ' kok', ' dong', ' banget', ' sih', ' tentu', ' lah', '!', '!!'],\n",
        "        'denied': ['', ' sih', ' kok', ' ah', ' deh', ' lah', '!', '!!'],\n",
        "    }\n",
        "\n",
        "    if intent in fillers and len(text.split()) <= 3:\n",
        "        # Add 0-2 random fillers\n",
        "        num_fillers = random.randint(0, 2)\n",
        "        for _ in range(num_fillers):\n",
        "            text += random.choice(fillers[intent])\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_augmentation(original, augmented):\n",
        "    \"\"\"Validate if augmentation is reasonable\"\"\"\n",
        "    # Check if augmentation is too different\n",
        "    if len(augmented.split()) < len(original.split()) * 0.5:\n",
        "        return False\n",
        "\n",
        "    # Check if augmentation is just the original\n",
        "    if augmented.lower() == original.lower():\n",
        "        return False\n",
        "\n",
        "    # Check if augmentation contains too many non-Indonesian characters\n",
        "    non_indo_pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!\\'\"-:;()[\\]{}]')\n",
        "    if len(non_indo_pattern.findall(augmented)) > 2:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def batch_paraphrase(model, tokenizer, sentences, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Process paraphrasing in batches\"\"\"\n",
        "    if not sentences or model is None or tokenizer is None:\n",
        "        return []\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i+batch_size]\n",
        "        inputs = tokenizer([\"paraphrase: \" + text + \" </s>\" for text in batch],\n",
        "                         padding='longest', truncation=True, max_length=128,\n",
        "                         return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for inference\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "                do_sample=True,\n",
        "                top_k=200,\n",
        "                top_p=0.95,\n",
        "                early_stopping=True,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "        decoded = [tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
        "                  for j in range(len(outputs))]\n",
        "        results.extend(decoded)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Combined augmentation strategies\n",
        "def augment_text(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply multiple augmentation techniques based on class needs\"\"\"\n",
        "    # Available methods\n",
        "    methods = {\n",
        "        'synonym': replace_with_synonym,\n",
        "        'back_translate': back_translate,\n",
        "        'typo': add_typo,\n",
        "        'deletion': random_deletion,\n",
        "        'swap': random_swap,\n",
        "        'phonetic': phonetic_augmentation,\n",
        "        'common_phrase': add_common_phrase,\n",
        "        'slang': lambda t: apply_slang_typo(t, intent, intensity),\n",
        "        'char_noise': lambda t: character_noise(t, intensity),\n",
        "        'short_text': lambda t: short_text_augmentation(t, intent)\n",
        "    }\n",
        "\n",
        "    # Choose augmentation methods based on text length and intent\n",
        "    text_length = len(text.split())\n",
        "\n",
        "    if text_length <= 3:  # Very short text\n",
        "        # For short texts, focus on slang and special augmentations\n",
        "        method_choices = ['slang', 'slang', 'phonetic', 'short_text', 'char_noise', 'synonym']\n",
        "        num_methods = min(3, int(intensity * 3))\n",
        "    else:  # Longer text\n",
        "        method_choices = ['slang', 'synonym', 'phonetic', 'back_translate', 'deletion', 'swap', 'char_noise', 'common_phrase']\n",
        "        num_methods = min(3, int(intensity * 2))\n",
        "\n",
        "    # Sample methods\n",
        "    selected_methods = random.sample(method_choices, k=num_methods)\n",
        "\n",
        "    # Apply selected methods in sequence\n",
        "    result = text\n",
        "    for method_name in selected_methods:\n",
        "        method = methods[method_name]\n",
        "        result = method(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def augment_data(text, intent):\n",
        "    \"\"\"Generate multiple augmentations for a text\"\"\"\n",
        "    methods = [\n",
        "        replace_with_synonym,\n",
        "        back_translate,\n",
        "        add_typo,\n",
        "        random_deletion,\n",
        "        random_swap,\n",
        "        phonetic_augmentation,\n",
        "        add_common_phrase,\n",
        "        lambda t: apply_slang_typo(t, intent, 1.0),\n",
        "        lambda t: character_noise(t, 1.0),\n",
        "        lambda t: short_text_augmentation(t, intent)\n",
        "    ]\n",
        "\n",
        "    augmented = set()\n",
        "    for method in methods:\n",
        "        try:\n",
        "            result = method(text)\n",
        "            if validate_augmentation(text, result):\n",
        "                augmented.add(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying {method.__name__}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return list(augmented)\n",
        "\n",
        "def balance_samples(results_by_intent, target_samples_per_class, original_counts):\n",
        "    \"\"\"\n",
        "    Balance samples by intent, ensuring a good mix of original and augmented data:\n",
        "    1. Keep all original samples\n",
        "    2. Add augmentations until target is reached\n",
        "    3. Limit excess samples if over target\n",
        "    \"\"\"\n",
        "    balanced_results = defaultdict(list)\n",
        "\n",
        "    # For each intent\n",
        "    for intent, samples in results_by_intent.items():\n",
        "        # Count of original samples for this intent\n",
        "        orig_count = original_counts.get(intent, 0)\n",
        "\n",
        "        # Current total count\n",
        "        current_count = len(samples)\n",
        "\n",
        "        if current_count <= target_samples_per_class:\n",
        "            # If sample count is below target, use all samples\n",
        "            balanced_results[intent] = samples\n",
        "        else:\n",
        "            # If over target, balance original vs augmented\n",
        "            # Ensure we keep all original data (priority)\n",
        "            original_data = samples[:orig_count]\n",
        "\n",
        "            # Add original data\n",
        "            balanced_results[intent].extend(original_data)\n",
        "\n",
        "            # Calculate how many augmented samples we can add\n",
        "            remaining_slots = target_samples_per_class - orig_count\n",
        "\n",
        "            if remaining_slots > 0:\n",
        "                # Get augmented samples (everything after original data)\n",
        "                augmented_data = samples[orig_count:]\n",
        "                # Randomize selection\n",
        "                random.shuffle(augmented_data)\n",
        "                # Add only what we need to reach target\n",
        "                balanced_results[intent].extend(augmented_data[:remaining_slots])\n",
        "\n",
        "    return balanced_results\n",
        "\n",
        "def plot_distribution(data, title):\n",
        "    \"\"\"Plot distribution of samples by intent\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    data['intent'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Intent\")\n",
        "    plt.ylabel(\"Jumlah Sampel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =========[ MAIN PROCESS ]=========\n",
        "def main():\n",
        "    \"\"\"Main process for dataset augmentation\"\"\"\n",
        "    # Set up file paths based on chosen data type\n",
        "    if DATA_TYPE == \"train\":\n",
        "        OUTPUT_FILE = \"train.csv\"\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "    elif DATA_TYPE == \"val\":\n",
        "        # If different paths needed for validation\n",
        "        OUTPUT_FILE = \"val.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"val\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"val\")\n",
        "    elif DATA_TYPE == \"test\":\n",
        "        # If different paths needed for test\n",
        "        OUTPUT_FILE = \"test.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"test\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"test\")\n",
        "    else:\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_dataset(input_file)\n",
        "\n",
        "    # Initialize paraphrase model if enabled\n",
        "    model, tokenizer = initialize_paraphrase_model()\n",
        "\n",
        "    # Count original samples per intent\n",
        "    intent_counts = Counter(df['intent'])\n",
        "    print(\"Original class distribution:\")\n",
        "    for intent, count in intent_counts.items():\n",
        "        print(f\"  {intent}: {count}\")\n",
        "\n",
        "    # Calculate augmentation factors for balancing\n",
        "    augmentation_factors = {}\n",
        "    for intent, count in intent_counts.items():\n",
        "        if count >= TARGET_SAMPLES_PER_CLASS:\n",
        "            augmentation_factors[intent] = 1  # Minimum factor\n",
        "        else:\n",
        "            factor = max(1, min(10, TARGET_SAMPLES_PER_CLASS / count))\n",
        "            augmentation_factors[intent] = factor\n",
        "\n",
        "    print(\"\\nAugmentation factors:\")\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        print(f\"  {intent}: {factor:.2f}x\")\n",
        "\n",
        "    # Start augmentation process\n",
        "    print(\"Starting balanced augmentation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    augmented_results = defaultdict(list)\n",
        "    paraphrase_candidates = defaultdict(list)\n",
        "\n",
        "    # First, add all original data\n",
        "    for _, row in df.iterrows():\n",
        "        intent = row['intent']\n",
        "        text = row['text']\n",
        "        augmented_results[intent].append(text)\n",
        "\n",
        "    # Then determine augmentation targets for each intent\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        original_count = intent_counts[intent]\n",
        "        intent_df = df[df['intent'] == intent]\n",
        "\n",
        "        for _, row in tqdm(intent_df.iterrows(), desc=f\"Augmenting '{intent}'\", total=len(intent_df)):\n",
        "            text = row['text']\n",
        "\n",
        "            # Calculate needed augmentations for this sample\n",
        "            num_augmentations = max(\n",
        "                MIN_AUGMENTATIONS_PER_SAMPLE,\n",
        "                min(MAX_AUGMENTATIONS_PER_SAMPLE, int(factor * 2))\n",
        "            )\n",
        "\n",
        "            # Regular augmentations\n",
        "            for _ in range(num_augmentations):\n",
        "                # Higher intensity for smaller classes\n",
        "                intensity = 1.0 + (factor - 1) * 0.5  # Scale 1.0-5.0 based on factor\n",
        "                aug_text = augment_text(text, intent, intensity)\n",
        "\n",
        "                if aug_text.lower() != text.lower():\n",
        "                    augmented_results[intent].append(aug_text)\n",
        "\n",
        "            # Add sample for paraphrasing if enabled\n",
        "            if USE_PARAPHRASE_MODEL and model is not None:\n",
        "                # More paraphrasing for under-represented classes\n",
        "                paraphrase_prob = min(0.8, factor * 0.2)\n",
        "                if random.random() < paraphrase_prob:\n",
        "                    paraphrase_candidates[intent].append(text)\n",
        "\n",
        "    # Process paraphrases in batches by intent\n",
        "    if USE_PARAPHRASE_MODEL and model is not None:\n",
        "        for intent, sentences in paraphrase_candidates.items():\n",
        "            if not sentences:\n",
        "                continue\n",
        "\n",
        "            print(f\"Paraphrasing {len(sentences)} sentences for intent '{intent}'...\")\n",
        "            paraphrased = batch_paraphrase(model, tokenizer, sentences)\n",
        "\n",
        "            # Add paraphrased results\n",
        "            for original, paraphrase in zip(sentences, paraphrased):\n",
        "                if original.lower() != paraphrase.lower():\n",
        "                    augmented_results[intent].append(paraphrase)\n",
        "\n",
        "    # Balance class distribution\n",
        "    print(\"Balancing class distribution...\")\n",
        "    balanced_results = balance_samples(augmented_results, TARGET_SAMPLES_PER_CLASS, intent_counts)\n",
        "\n",
        "    # Combine all results\n",
        "    final_results = []\n",
        "    for intent, texts in balanced_results.items():\n",
        "        for text in texts:\n",
        "            final_results.append((intent, text))\n",
        "\n",
        "    # Save results\n",
        "    print(\"Final class distribution:\")\n",
        "    result_counts = defaultdict(int)\n",
        "    for intent, _ in final_results:\n",
        "        result_counts[intent] += 1\n",
        "\n",
        "    for intent, count in result_counts.items():\n",
        "        print(f\"  {intent}: {count}\")\n",
        "\n",
        "    aug_df = pd.DataFrame(final_results, columns=['intent', 'text'])\n",
        "    aug_df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
        "    aug_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"[] Selesai dalam {time.time() - start_time:.2f} detik!\")\n",
        "    print(f\"Original: {len(df)} | Total setelah augmentasi: {len(aug_df)}  {output_file}\")\n",
        "\n",
        "    # Plot distributions\n",
        "    print(\"\\n Distribusi Sebelum Augmentasi:\")\n",
        "    df_original = df[['text', 'intent']]\n",
        "    plot_distribution(df_original, \"Distribusi Intent Sebelum Augmentasi\")\n",
        "\n",
        "    print(\"\\n Distribusi Setelah Augmentasi:\")\n",
        "    plot_distribution(aug_df, \"Distribusi Intent Setelah Augmentasi (Seimbang)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUYgABG0S6lf",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Augmentation for Indonesian NLP - Improved Version\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import Levenshtein as lev  # For better text difference calculation\n",
        "\n",
        "# Download WordNet data (if not already downloaded)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# =========[ KONFIGURASI ]=========\n",
        "INPUT_FILE = \"/content/custom_test_dataset.csv\" # @param {\"type\":\"string\"}\n",
        "DATA_TYPE = \"test\" # @param [\"train\", \"val\", \"test\"]\n",
        "TARGET_SAMPLES_PER_CLASS = 600    # @param {type:\"integer\"} Target jumlah sampel per kelas\n",
        "NOISE_INTENSITY = 0.7             # @param {type:\"number\"} Control how aggressive augmentations are (0.1-1.0)\n",
        "USE_PARAPHRASE_MODEL = True       # @param {type:\"boolean\"} Aktifkan atau matikan paraphrase\n",
        "USE_BACK_TRANSLATION = False       # @param {type:\"boolean\"} Aktifkan atau matikan back-translation\n",
        "MIN_AUGMENTATIONS_PER_SAMPLE = 1  # @param {type:\"integer\"} Minimum augmentasi per sampel asli\n",
        "MAX_AUGMENTATIONS_PER_SAMPLE = 10  # @param {type:\"integer\"} Maximum augmentasi per sampel asli (reduced from 10)\n",
        "BATCH_SIZE = 16                   # @param {type:\"integer\"} Untuk batch processing\n",
        "\n",
        "# =========[ IMPROVED SYNONYM DICTIONARY ]=========\n",
        "# Kamus sinonim Indonesia untuk kata-kata umum\n",
        "id_synonyms = {\n",
        "    # Kata-kata terkait sapaan\n",
        "    \"saya\": [\"aku\", \"gue\", \"gua\", \"hamba\", \"beta\"],\n",
        "    \"kamu\": [\"anda\", \"engkau\", \"kalian\", \"elu\", \"dikau\"],\n",
        "    \"hai\": [\"halo\", \"hello\", \"hi\", \"hey\", \"hola\"],\n",
        "    \"selamat\": [\"slamat\", \"met\", \"congratz\"],\n",
        "    \"pagi\": [\"morning\", \"subuh\"],\n",
        "    \"siang\": [\"dzuhur\", \"sore\"],\n",
        "    \"malam\": [\"malem\", \"night\"],\n",
        "\n",
        "    # Kata-kata umum\n",
        "    \"tidak\": [\"tak\", \"tiada\", \"bukan\", \"enggak\", \"nggak\", \"gak\", \"kagak\"],\n",
        "    \"iya\": [\"ya\", \"yoi\", \"yup\", \"yo\", \"oke\", \"ok\", \"betul\"],\n",
        "    \"bagaimana\": [\"gimana\", \"bagaimane\", \"gmn\", \"bgmn\"],\n",
        "    \"mengapa\": [\"kenapa\", \"knp\", \"knapa\", \"ngapa\"],\n",
        "    \"karena\": [\"sebab\", \"lantaran\", \"gara-gara\"],\n",
        "    \"sangat\": [\"banget\", \"amat\", \"sekali\", \"sungguh\"],\n",
        "    \"bisa\": [\"dapat\", \"mampu\", \"sanggup\"],\n",
        "    \"mungkin\": [\"barangkali\", \"kali\", \"kayaknya\", \"kayanya\"],\n",
        "    \"terima kasih\": [\"makasih\", \"tks\", \"thx\", \"thanks\", \"tengkyu\", \"tengkiu\"],\n",
        "    \"tolong\": [\"bantu\", \"bantuin\", \"tlg\", \"help\"],\n",
        "    \"mau\": [\"ingin\", \"pengen\", \"berkenan\"],\n",
        "    \"pergi\": [\"berangkat\", \"cabut\", \"start\", \"jalan\"],\n",
        "    \"lihat\": [\"tengok\", \"liat\", \"ngeliat\", \"mantau\"],\n",
        "    \"cari\": [\"mencari\", \"nyari\", \"telusuri\"]\n",
        "}\n",
        "\n",
        "# =========[ INTENT-SPECIFIC SLANG DICTIONARIES ]=========\n",
        "# Kata gaul umum untuk semua intent\n",
        "common_slang = {\n",
        "    'tidak': 'gak',\n",
        "    'iya': 'yoi',\n",
        "    'terima kasih': 'makasih',\n",
        "    'saya': 'gw',\n",
        "    'kamu': 'lo',\n",
        "    'sedang': 'lagi',\n",
        "    'bagaimana': 'gimana',\n",
        "    'begitu': 'gitu',\n",
        "    'bisa': 'bsa',\n",
        "    'akan': 'bakal',\n",
        "    'untuk': 'buat',\n",
        "    'tahu': 'tau',\n",
        "    'apakah': 'apa',\n",
        "    'mengapa': 'kenapa',\n",
        "    'selamat': 'selamet',\n",
        "    'dengan': 'dgn',\n",
        "    'sangat': 'banget'\n",
        "}\n",
        "\n",
        "# Kamus kata gaul khusus intent\n",
        "intent_slang = {\n",
        "    'jam_layanan': {\n",
        "        'perpustakaan': 'perpus',\n",
        "        'buka': 'open',\n",
        "        'tutup': 'close',\n",
        "        'sampai': 'sampe',\n",
        "        'jam': 'jm',\n",
        "        'informasi': 'info',\n",
        "        'hari ini': 'hr ini',\n",
        "        'kapan': 'kpn',\n",
        "        'jadwal': 'jdwl',\n",
        "        'operasional': 'ops',\n",
        "        'layanan': 'lyn',\n",
        "        'masih': 'msih',\n",
        "        'minggu': 'mg',\n",
        "        'hari': 'hri',\n",
        "        'pukul': 'pkl',\n",
        "    },\n",
        "    'cari_buku': {\n",
        "        'mencari': 'nyari',\n",
        "        'mau mencari': 'mau nyari',\n",
        "        'ingin mencari': 'pengen nyari',\n",
        "        'tolong carikan': 'cariin',\n",
        "        'tolong bantu cari': 'bantuin cari',\n",
        "        'mencarikan': 'cariin',\n",
        "        'buku': 'book',\n",
        "        'butuh': 'need',\n",
        "        'melihat': 'liat',\n",
        "        'daftar': 'list',\n",
        "        'akses': 'akses',\n",
        "        'temukan': 'nemu',\n",
        "        'mencoba': 'nyoba',\n",
        "        'mengakses': 'akses',\n",
        "        'gunakan': 'make use',\n",
        "        'fitur': 'fitr',\n",
        "        'pencarian': 'search',\n",
        "        'referensi': 'ref',\n",
        "        'bantuan': 'bntuan',\n",
        "        'dimana': 'dmn',\n",
        "        'cek': 'check',\n",
        "        'lihat-lihat': 'liat2',\n",
        "        'cari': 'search',\n",
        "        'mencari buku': 'nyari book',\n",
        "    },\n",
        "    'greeting': {\n",
        "        'halo': 'haloo',\n",
        "        'hai': 'hay',\n",
        "        'hello': 'helo',\n",
        "        'selamat pagi': 'slmt pagi',\n",
        "        'selamat siang': 'slmt siang',\n",
        "        'selamat sore': 'slmt sore',\n",
        "        'selamat malam': 'slmt malam',\n",
        "        'apa kabar': 'apa kbr',\n",
        "        'assalamualaikum': 'asswrwb',\n",
        "        'permisi': 'permizz',\n",
        "        'hai bot': 'hey bot',\n",
        "        'bot': 'bt',\n",
        "        'selamat datang': 'slmt dtg',\n",
        "        # Additional variations for greeting\n",
        "        'halo selamat pagi': 'hai morning',\n",
        "        'hai selamat siang': 'helo siang',\n",
        "        'met pagi': 'morning',\n",
        "        'pagi': 'pgi',\n",
        "        'siang': 'siang boss',\n",
        "    },\n",
        "    'goodbye': {\n",
        "        'terima kasih': 'makasih',\n",
        "        'goodbye': 'gudbai',\n",
        "        'makasih': 'mksh',\n",
        "        'makasih ya': 'thx ya',\n",
        "        'sampai jumpa': 'sampe jmpa',\n",
        "        'dadah': 'daah',\n",
        "        'bye': 'byee',\n",
        "        'sampai nanti': 'sampe ntar',\n",
        "        'see you': 'cu',\n",
        "        'thanks': 'thx',\n",
        "        'thank you': 'tq',\n",
        "        'sekian': 'skian',\n",
        "        'itu saja': 'itu aj',\n",
        "        # Additional variations for goodbye\n",
        "        'ok makasih': 'ok thx',\n",
        "        'terima kasih banyak': 'thanks banget',\n",
        "        'makasih atas bantuannya': 'thx for helping',\n",
        "        'sampai bertemu lagi': 'see u later',\n",
        "        'selamat tinggal': 'bye bye',\n",
        "    },\n",
        "    'confirm': {\n",
        "        'betul': 'btl',\n",
        "        'setuju': 'stju',\n",
        "        'bener': 'bnr',\n",
        "        'iya benar': 'ya bnr',\n",
        "        'okey': 'okeyy',\n",
        "        'ok deh': 'okedeh',\n",
        "        # Additional variations for confirm\n",
        "        'tentu saja': 'tentu',\n",
        "        'saya setuju': 'aku setuju',\n",
        "        'benar sekali': 'bener banget',\n",
        "        'ya betul': 'yup betul',\n",
        "        'tentu boleh': 'boleh dong',\n",
        "        'setuju sekali': 'sangat setuju',\n",
        "    },\n",
        "    'denied': {\n",
        "        'tidak mau': 'gak mau',\n",
        "        'ga mau': 'gk mw',\n",
        "        'tidak setuju': 'gak setuju',\n",
        "        'saya tidak': 'aku ga',\n",
        "        'nggak perlu': 'ga perlu',\n",
        "        'ga usah': 'rasah',\n",
        "        'tidak perlu': 'gak usah',\n",
        "        'no': 'nope',\n",
        "        # Additional variations for denied\n",
        "        'saya tidak setuju': 'aku gak setuju',\n",
        "        'tidak bisa': 'ga bisa',\n",
        "        'tidak boleh': 'gak boleh',\n",
        "        'jangan': 'jgn',\n",
        "        'maaf tidak': 'sorry no',\n",
        "        'tidak begitu': 'gak gitu',\n",
        "    }\n",
        "}\n",
        "\n",
        "# =========[ PHONETIC AUGMENTATION DICTIONARY ]=========\n",
        "phonetic_dict = {\n",
        "        # Greeting related\n",
        "        \"saya\": [\"sy\", \"saia\", \"ane\", \"ana\", \"w\", \"gw\", \"q\", \"aq\"],\n",
        "        \"kamu\": [\"km\", \"kamyu\", \"u\", \"lo\", \"lu\", \"l\", \"ngana\", \"sampeyan\", \"antum\", \"ente\"],\n",
        "        \"halo\": [\"hlo\", \"hallo\", \"helo\", \"haloo\", \"hellow\", \"hy\", \"hyy\", \"p\", \"ping\"],\n",
        "        \"selamat\": [\"slmt\", \"slamat\", \"met\", \"slam\"],\n",
        "        \"pagi\": [\"pgi\", \"morning\", \"pg\", \"subuh\"],\n",
        "        \"siang\": [\"siang\", \"afternoon\", \"siank\", \"siyang\"],\n",
        "        \"malam\": [\"mlm\", \"malem\", \"mlem\", \"night\", \"evening\", \"mlem\", \"mlm\"],\n",
        "        \"apa\": [\"ap\", \"ape\", \"apah\", \"pa\"],\n",
        "        \"kabar\": [\"kbr\", \"kabare\", \"kbar\", \"kabbar\"],\n",
        "        \"gimana\": [\"gmn\", \"bgmn\", \"gmana\", \"gimane\", \"gmn\"],\n",
        "\n",
        "        # Goodbye related\n",
        "        \"sampai\": [\"smp\", \"sampe\", \"smpe\", \"smpei\", \"sampeyan\"],\n",
        "        \"jumpa\": [\"jpa\", \"jmpa\", \"ktmu\", \"jumpe\"],\n",
        "        \"dadah\": [\"byebye\", \"bye\", \"bay\", \"byee\", \"bbye\", \"bye2\", \"dadah\"],\n",
        "        \"pamit\": [\"pmt\", \"pamitt\", \"off\", \"out\", \"cabut\", \"cbut\"],\n",
        "        \"tinggal\": [\"tgl\", \"tnggal\", \"tinggel\", \"tggal\", \"tinggelin\"],\n",
        "        \"duluan\": [\"dlu\", \"duluan\", \"dluan\", \"dluan ya\", \"ahead\"],\n",
        "        \"pergi\": [\"pgi\", \"pegi\", \"prgi\", \"out\", \"keluar\"],\n",
        "        \"pulang\": [\"plg\", \"plng\", \"balik\", \"blk\", \"mudik\"],\n",
        "\n",
        "        # Confirm related\n",
        "        \"ya\": [\"y\", \"yah\", \"iye\", \"yoi\", \"yups\", \"yes\", \"yess\", \"yesss\", \"okey\", \"okeh\", \"oks\"],\n",
        "        \"setuju\": [\"stju\", \"acc\", \"accept\", \"approved\", \"approve\", \"deal\", \"oke\", \"ok\", \"sip\"],\n",
        "        \"benar\": [\"bnr\", \"bner\", \"bener\", \"bnr\", \"yoi\", \"correct\"],\n",
        "        \"sudah\": [\"sdh\", \"dah\", \"udh\", \"done\", \"wes\", \"uwes\", \"udah\", \"sdah\"],\n",
        "        \"bisa\": [\"bs\", \"bsa\", \"biza\", \"bsa\", \"biza\", \"ok\"],\n",
        "        \"pasti\": [\"pst\", \"psti\", \"pastii\", \"sure\", \"certain\"],\n",
        "        \"siap\": [\"sp\", \"ready\", \"sip\", \"sp\", \"roger\", \"on\", \"online\"],\n",
        "        \"jadi\": [\"jd\", \"jdi\", \"jdnya\", \"jdiin\", \"proceed\"],\n",
        "        \"lanjut\": [\"lnjt\", \"lanjt\", \"next\", \"go\"],\n",
        "        \"mantap\": [\"mntap\", \"mantab\", \"mntb\", \"top\", \"mantul\", \"josss\", \"kerennn\"],\n",
        "        \"bagus\": [\"bgs\", \"bgus\", \"nice\", \"naiss\", \"keren\", \"top\"],\n",
        "\n",
        "        # Denied related\n",
        "        \"tidak\": [\"tdk\", \"gak\", \"ga\", \"g\", \"nggak\", \"ngga\", \"nope\", \"no\", \"kagak\", \"kaga\", \"kgk\"],\n",
        "        \"jangan\": [\"jgn\", \"jngn\", \"don't\", \"dont\", \"jgn\", \"ga usah\", \"tdk usah\", \"gausa\", \"gausah\"],\n",
        "        \"belum\": [\"blm\", \"blom\", \"belom\", \"not yet\", \"durung\", \"durong\", \"belm\"],\n",
        "        \"batal\": [\"btl\", \"cancel\", \"cansel\", \"urungkan\", \"batalin\", \"gajadi\"],\n",
        "        \"maaf\": [\"sorry\", \"sori\", \"maf\", \"maap\", \"maaaaf\", \"mrff\", \"sry\", \"srry\"],\n",
        "        \"menolak\": [\"tlk\", \"reject\", \"decline\", \"dtolak\", \"nolak\", \"gak mau\", \"gamau\"],\n",
        "        \"mustahil\": [\"impossible\", \"ga mungkin\", \"g mungkin\", \"tdk mungkin\", \"gak bs\"],\n",
        "        \"salah\": [\"slh\", \"wrong\", \"error\", \"eror\", \"salh\", \"fail\"],\n",
        "        \"gagal\": [\"ggl\", \"fail\", \"failed\", \"error\", \"gagak\", \"failll\"],\n",
        "\n",
        "        # Common words\n",
        "        \"terima\": [\"trma\", \"thanks\", \"thx\", \"trims\", \"tq\", \"tyvm\", \"makasih\", \"mksih\"],\n",
        "        \"kasih\": [\"ksh\", \"ksih\", \"thx\", \"makasih\", \"mksih\", \"thanks\"],\n",
        "        \"tolong\": [\"tlng\", \"help\", \"tlg\", \"tulung\", \"bantu\", \"bantuin\"],\n",
        "        \"please\": [\"plz\", \"plis\", \"pliss\", \"plisss\", \"pliissss\", \"tolong\"],\n",
        "        \"besok\": [\"bsk\", \"bsok\", \"besok\", \"tmrw\", \"esok\", \"besuk\"],\n",
        "        \"waktu\": [\"wkt\", \"waktu\", \"time\", \"tm\", \"jam\"],\n",
        "        \"cukup\": [\"ckp\", \"enough\", \"cukuppp\", \"cukups\", \"ckup\"],\n",
        "        \"melihat\": [\"lihat\", \"liat\", \"look\", \"see\", \"watching\", \"ngeliat\"],\n",
        "        \"alasan\": [\"alsan\", \"reason\", \"why\", \"alesan\", \"alsn\"],\n",
        "        \"untuk\": [\"utk\", \"buat\", \"bwt\", \"4\", \"tuk\", \"2\", \"to\"],\n",
        "        \"melakukan\": [\"lakukan\", \"do\", \"lakuin\", \"melakuin\", \"ngerjain\"],\n",
        "        \"ini\": [\"ni\", \"this\", \"these\", \"iki\", \"nih\", \"ne\"],\n",
        "        \"kita\": [\"kta\", \"we\", \"us\", \"w\", \"kt\"],\n",
        "        \"hari\": [\"hr\", \"day\", \"hri\", \"days\", \"dayy\", \"harii\"],\n",
        "        \"bro\": [\"broh\", \"brow\", \"brother\", \"mas\", \"bang\", \"bor\", \"omm\"],\n",
        "        \"sis\": [\"sist\", \"sister\", \"mbak\", \"mba\", \"nte\", \"ceu\", \"teh\"]\n",
        "    }\n",
        "\n",
        "# =========[ PROTECTED WORDS BY INTENT ]=========\n",
        "# Words that should not be altered for each intent to preserve meaning\n",
        "protected_intent_words = {\n",
        "    'jam_layanan': ['jam', 'buka', 'tutup', 'perpustakaan', 'layanan', 'kapan'],\n",
        "    'cari_buku': ['buku', 'cari', 'judul', 'penulis', 'isbn', 'kategori'],\n",
        "    'greeting': ['halo', 'hai', 'selamat', 'pagi', 'siang', 'malam'],\n",
        "    'goodbye': ['selamat', 'tinggal', 'sampai', 'jumpa', 'terima', 'kasih'],\n",
        "    'confirm': ['iya', 'betul', 'benar', 'setuju', 'ok'],\n",
        "    'denied': ['tidak', 'bukan', 'salah', 'jangan']\n",
        "}\n",
        "\n",
        "\n",
        "# =========[ READ & VALIDATE FILE ]=========\n",
        "def read_dataset(file_path):\n",
        "    \"\"\"Membaca dataset dari file CSV atau XLSX\"\"\"\n",
        "    print(f\"Loading dataset: {file_path}\")\n",
        "\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_ext == '.xlsx':\n",
        "        print(f\"Detected Excel file: {file_path}\")\n",
        "        df = pd.read_excel(file_path)\n",
        "        # Konversi ke CSV untuk kompatibilitas\n",
        "        csv_path = file_path.replace('.xlsx', '.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Converted Excel file to CSV: {csv_path}\")\n",
        "    elif file_ext == '.csv':\n",
        "        print(f\"Detected CSV file: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Format file tidak didukung: {file_ext}. Harap gunakan file CSV atau XLSX.\")\n",
        "\n",
        "    df = df.dropna()\n",
        "    print(f\"Dataset dimuat dengan {len(df)} baris\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========[ INITIALIZE PARAPHRASE MODEL IF NEEDED ]=========\n",
        "def initialize_paraphrase_model():\n",
        "    \"\"\"Initialize paraphrase model if enabled\"\"\"\n",
        "    if USE_PARAPHRASE_MODEL:\n",
        "        print(\"Loading paraphrase model...\")\n",
        "        start_time = time.time()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"Model loaded in {time.time() - start_time:.2f} seconds. Using device: {device}\")\n",
        "        return model, tokenizer\n",
        "    return None, None\n",
        "\n",
        "# =========[ AUGMENTATION METHODS ]=========\n",
        "def get_better_synonym(word):\n",
        "    \"\"\"Get synonym from custom dictionary or return the original word\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in id_synonyms:\n",
        "        synonyms = id_synonyms[word_lower]\n",
        "        return random.choice(synonyms)\n",
        "    return word\n",
        "\n",
        "def replace_with_synonym(sentence):\n",
        "    \"\"\"Replace words with synonyms while preserving capitalization\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    # Limit the number of words to replace to avoid excessive changes\n",
        "    num_to_replace = min(2, max(1, int(len(words) * 0.2)))\n",
        "    indices_to_replace = random.sample(range(len(words)), k=min(num_to_replace, len(words)))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if i in indices_to_replace:\n",
        "            synonym = get_better_synonym(word)\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper() and synonym:\n",
        "                synonym = synonym[0].upper() + synonym[1:]\n",
        "            new_words.append(synonym)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def back_translate(sentence):\n",
        "    \"\"\"Translate to English and back to Indonesian with safety checks\"\"\"\n",
        "    if not USE_BACK_TRANSLATION:\n",
        "        return sentence\n",
        "\n",
        "    # Skip very short sentences\n",
        "    if len(sentence.split()) < 3:\n",
        "        return sentence\n",
        "\n",
        "    try:\n",
        "        # First to English\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(sentence)\n",
        "        # Then back to Indonesian\n",
        "        back_translated = GoogleTranslator(source='en', target='id').translate(translated)\n",
        "\n",
        "        # Safety checks\n",
        "        if back_translated and len(back_translated.split()) >= len(sentence.split()) * 0.7:\n",
        "            # Calculate how different the result is\n",
        "            similarity = 1 - (lev.distance(sentence.lower(), back_translated.lower()) / max(len(sentence), len(back_translated)))\n",
        "            # If too different or too similar, return original\n",
        "            if similarity < 0.3 or similarity > 0.9:\n",
        "                return sentence\n",
        "            return back_translated\n",
        "        return sentence\n",
        "    except Exception:\n",
        "        return sentence\n",
        "\n",
        "def add_typo(sentence):\n",
        "    \"\"\"Add a single typo by replacing a character, with reduced probability\"\"\"\n",
        "    # Skip for very short sentences or with low global noise setting\n",
        "    if len(sentence) < 10 or random.random() > NOISE_INTENSITY:\n",
        "        return sentence\n",
        "\n",
        "    chars = list(sentence)\n",
        "    if len(chars) > 3:\n",
        "        # Try to find a good character to modify (not first or last character)\n",
        "        candidates = [i for i in range(1, len(chars)-1) if chars[i].isalpha()]\n",
        "        if candidates:\n",
        "            idx = random.choice(candidates)\n",
        "            # Get neighboring letters on keyboard for more realistic typos\n",
        "            keyboard_neighbors = {\n",
        "                'q': 'wsa', 'w': 'qeasd', 'e': 'wrsdf', 'r': 'etdfg',\n",
        "                't': 'ryfgh', 'y': 'tughj', 'u': 'yihjk', 'i': 'uojkl',\n",
        "                'o': 'ipkl', 'p': 'ol',\n",
        "                'a': 'qwszx', 's': 'awedcxz', 'd': 'serfcvx', 'f': 'drtgvbc',\n",
        "                'g': 'ftyhvbn', 'h': 'gyujbnm', 'j': 'huiknm', 'k': 'jiolm',\n",
        "                'l': 'kop',\n",
        "                'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb',\n",
        "                'b': 'vghn', 'n': 'bhjm', 'm': 'njk'\n",
        "            }\n",
        "            char = chars[idx].lower()\n",
        "            if char in keyboard_neighbors:\n",
        "                chars[idx] = random.choice(keyboard_neighbors[char])\n",
        "\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def random_deletion(sentence, p=0.1):  # Reduced probability from 0.2\n",
        "    \"\"\"Delete words with probability p\"\"\"\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Don't delete from short sentences\n",
        "    if len(words) <= 4:\n",
        "        return sentence\n",
        "\n",
        "    # Don't delete too many words\n",
        "    max_deletions = max(1, int(len(words) * 0.1))\n",
        "    deletion_count = 0\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p or deletion_count >= max_deletions:\n",
        "            new_words.append(word)\n",
        "        else:\n",
        "            deletion_count += 1\n",
        "\n",
        "    # Make sure we don't delete everything\n",
        "    if not new_words:\n",
        "        return sentence\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    \"\"\"Swap n pairs of words\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) < 4:  # Don't swap in very short sentences\n",
        "        return sentence\n",
        "\n",
        "    # Limit swaps to just 1 for shorter sentences\n",
        "    if len(words) < 8:\n",
        "        n = 1\n",
        "\n",
        "    for _ in range(min(n, len(words)//3)):  # Reduced number of swaps\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def phonetic_augmentation(sentence):\n",
        "    \"\"\"Apply phonetic substitutions common in Indonesian chat\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    # Limit substitutions to maintain readability\n",
        "    max_substitutions = min(2, max(1, int(len(words) * 0.2)))\n",
        "    substitution_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        if word_lower in phonetic_dict and substitution_count < max_substitutions:\n",
        "            new_word = random.choice(phonetic_dict[word_lower])\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper():\n",
        "                new_word = new_word[0].upper() + new_word[1:]\n",
        "            new_words.append(new_word)\n",
        "            substitution_count += 1\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def apply_slang_typo(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply slang replacements with controllable intensity\"\"\"\n",
        "    # Combine common slang with intent-specific slang\n",
        "    slang_dict = common_slang.copy()\n",
        "    if intent in intent_slang:\n",
        "        slang_dict.update(intent_slang[intent])\n",
        "\n",
        "    # Create regex patterns from the slang dictionary\n",
        "    patterns = {\n",
        "        re.compile(rf'\\b{k}\\b', re.IGNORECASE): v for k, v in slang_dict.items()\n",
        "    }\n",
        "\n",
        "    # Apply only a few patterns based on intensity and text length\n",
        "    max_replacements = min(2, max(1, int(len(text.split()) * 0.2)))\n",
        "    patterns_to_use = random.sample(\n",
        "        list(patterns.items()),\n",
        "        k=min(max_replacements, int(len(patterns) * min(0.3, intensity * 0.5)))\n",
        "    )\n",
        "\n",
        "    for pattern, replacement in patterns_to_use:\n",
        "        text = pattern.sub(replacement, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def character_noise(text, intensity=1.0):\n",
        "    \"\"\"Add typos like character swaps, insertions, deletions with reduced intensity\"\"\"\n",
        "    # Skip for very short texts\n",
        "    if len(text) < 10 or random.random() > NOISE_INTENSITY:\n",
        "        return text\n",
        "\n",
        "    chars = list(text)\n",
        "    # Significantly reduce swap probability\n",
        "    swap_prob = min(0.03, intensity * 0.01)  # Lower from 0.1 to 0.03\n",
        "\n",
        "    # Limit to just one or two swaps per sentence\n",
        "    max_swaps = min(1, int(len(chars) * 0.05))\n",
        "    swap_count = 0\n",
        "\n",
        "    for i in range(len(chars)-1):\n",
        "        if random.random() < swap_prob and swap_count < max_swaps:\n",
        "            # Don't swap punctuation or spaces\n",
        "            if chars[i].isalpha() and chars[i+1].isalpha():\n",
        "                chars[i], chars[i+1] = chars[i+1], chars[i]\n",
        "                swap_count += 1\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "def add_common_phrase(sentence):\n",
        "    \"\"\"Add a common Indonesian chat phrase\"\"\"\n",
        "    # Skip for longer sentences\n",
        "    if len(sentence.split()) > 8:\n",
        "        return sentence\n",
        "\n",
        "    common_phrases = [\"sih\", \"ya\", \"dong\", \"cuy\", \"bro\", \"lah\", \"deh\"]\n",
        "    return sentence + \" \" + random.choice(common_phrases)\n",
        "\n",
        "def short_text_augmentation(text, intent):\n",
        "    \"\"\"Special augmentation for very short texts like greetings and goodbyes\"\"\"\n",
        "    # For very short texts, add filler words or expressions\n",
        "    fillers = {\n",
        "        'greeting': ['', ' ya', ' kak', ' min', ' gan', ' bro', ' sis', ' admin', '!'],\n",
        "        'goodbye': ['', ' ya', ' kak', ' min', ' semuanya', '!'],\n",
        "        'confirm': ['', ' kok', ' dong', ' banget', ' sih', ' tentu', ' lah', '!'],\n",
        "        'denied': ['', ' sih', ' kok', ' ah', ' deh', ' lah', '!'],\n",
        "    }\n",
        "\n",
        "    if intent in fillers and len(text.split()) <= 3:\n",
        "        # Add just one filler\n",
        "        if random.random() < 0.7:  # 70% chance to add filler\n",
        "            text += random.choice(fillers[intent])\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_augmentation(original, augmented):\n",
        "    \"\"\"Validate if augmentation is reasonable with stricter requirements\"\"\"\n",
        "    # Skip if no change\n",
        "    if augmented.lower() == original.lower():\n",
        "        return False\n",
        "\n",
        "    # Calculate word count difference\n",
        "    orig_words = original.split()\n",
        "    aug_words = augmented.split()\n",
        "\n",
        "    # Check if length is reasonable\n",
        "    if len(aug_words) < len(orig_words) * 0.6 or len(aug_words) > len(orig_words) * 1.4:\n",
        "        return False\n",
        "\n",
        "    # Calculate text similarity using Levenshtein distance\n",
        "    normalized_distance = lev.distance(original.lower(), augmented.lower()) / max(len(original), len(augmented))\n",
        "    # If too similar or too different, reject\n",
        "    if normalized_distance < 0.03 or normalized_distance > 0.5:\n",
        "        return False\n",
        "\n",
        "    # Check for excessive non-standard characters\n",
        "    non_indo_pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!\\'\"-:;()[\\]{}]')\n",
        "    if len(non_indo_pattern.findall(augmented)) > 3:\n",
        "        return False\n",
        "\n",
        "    # Check if individual words have been mangled too much\n",
        "    if len(orig_words) == len(aug_words):\n",
        "        word_changes = 0\n",
        "        for i in range(len(orig_words)):\n",
        "            # Check word edit distance\n",
        "            if len(orig_words[i]) > 3 and lev.distance(orig_words[i], aug_words[i]) > len(orig_words[i]) * 0.5:\n",
        "                word_changes += 1\n",
        "\n",
        "        # Reject if too many words changed significantly\n",
        "        if word_changes / len(orig_words) > 0.4:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def batch_paraphrase(model, tokenizer, sentences, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Process paraphrasing in batches\"\"\"\n",
        "    if not sentences or model is None or tokenizer is None:\n",
        "        return []\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i+batch_size]\n",
        "        inputs = tokenizer([\"paraphrase: \" + text + \" </s>\" for text in batch],\n",
        "                         padding='longest', truncation=True, max_length=128,\n",
        "                         return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for inference\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "                do_sample=True,\n",
        "                top_k=200,  # Reduce from 200 to 120 for more conservative output\n",
        "                top_p=0.98,\n",
        "                temperature=NOISE_INTENSITY + 0.3, # Added temperature control\n",
        "                early_stopping=False,\n",
        "                num_return_sequences=min(3, BATCH_SIZE // len(batch))\n",
        "            )\n",
        "\n",
        "        decoded = [tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
        "                  for j in range(len(outputs))]\n",
        "        results.extend(decoded)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Combined augmentation strategies\n",
        "def augment_text(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply multiple augmentation techniques based on class needs\"\"\"\n",
        "    # Scale intensity by global noise setting\n",
        "    intensity = intensity * NOISE_INTENSITY\n",
        "\n",
        "    # Protect intent-critical words\n",
        "    protected = []\n",
        "    if intent in protected_intent_words:\n",
        "        for word in protected_intent_words[intent]:\n",
        "            pattern = re.compile(rf'\\b{word}\\b', re.IGNORECASE)\n",
        "            for match in pattern.finditer(text):\n",
        "                # Replace with a temporary placeholder\n",
        "                placeholder = f\"__PROTECTED_{len(protected)}__\"\n",
        "                text = text[:match.start()] + placeholder + text[match.end():]\n",
        "                protected.append((placeholder, match.group(0)))\n",
        "\n",
        "    # Available methods - reorder by safety\n",
        "    methods = {\n",
        "        'synonym': replace_with_synonym,          # Safe\n",
        "        'back_translate': back_translate if USE_BACK_TRANSLATION else None,  # Generally safe\n",
        "        'slang': lambda t: apply_slang_typo(t, intent, intensity), # Safe if using known slang\n",
        "        'common_phrase': add_common_phrase,       # Safe\n",
        "        'short_text': lambda t: short_text_augmentation(t, intent), # Safe\n",
        "        'swap': random_swap,                      # Moderately safe\n",
        "        'deletion': random_deletion,              # Can be problematic\n",
        "        'phonetic': phonetic_augmentation,        # Can be problematic\n",
        "        'char_noise': lambda t: character_noise(t, intensity * 0.5),  # Reduced intensity\n",
        "        'typo': add_typo                          # Most problematic\n",
        "    }\n",
        "\n",
        "    # Remove None methods\n",
        "    methods = {k: v for k, v in methods.items() if v is not None}\n",
        "\n",
        "    # Choose augmentation methods based on text length and intent\n",
        "    text_length = len(text.split())\n",
        "\n",
        "    if text_length <= 3:  # Very short text\n",
        "        # For short texts, focus on safer methods\n",
        "        method_choices = ['slang', 'short_text', 'synonym', 'common_phrase']\n",
        "        num_methods = min(2, int(intensity * 2))  # Reduce number of transformations\n",
        "    else:  # Longer text\n",
        "        # Weight safer methods higher in the selection\n",
        "        method_choices = ['synonym', 'synonym', 'back_translate', 'slang', 'slang',\n",
        "                          'common_phrase', 'swap', 'deletion', 'phonetic', 'char_noise']\n",
        "        # Apply fewer transformations overall\n",
        "        num_methods = min(2, int(intensity * 1.5))\n",
        "\n",
        "    # Filter out any methods that aren't available (like back_translate if disabled)\n",
        "    method_choices = [m for m in method_choices if m in methods]\n",
        "\n",
        "    # Sample methods\n",
        "    if method_choices:\n",
        "        selected_methods = random.sample(method_choices, k=min(num_methods, len(method_choices)))\n",
        "    else:\n",
        "        selected_methods = []\n",
        "\n",
        "    # Apply selected methods in sequence\n",
        "    result = text\n",
        "    for method_name in selected_methods:\n",
        "        if method_name in methods:\n",
        "            method = methods[method_name]\n",
        "            result = method(result)\n",
        "\n",
        "    # Restore protected words\n",
        "    for placeholder, original in protected:\n",
        "        result = result.replace(placeholder, original)\n",
        "\n",
        "    return result\n",
        "\n",
        "def augment_data(text, intent):\n",
        "    \"\"\"Generate multiple augmentations for a text\"\"\"\n",
        "    methods = [\n",
        "        replace_with_synonym,\n",
        "        back_translate if USE_BACK_TRANSLATION else None,\n",
        "        lambda t: augment_text(t, intent, 1.0)  # Use combined approach\n",
        "    ]\n",
        "\n",
        "    # Remove None methods\n",
        "    methods = [m for m in methods if m is not None]\n",
        "\n",
        "    augmented = set()\n",
        "    for method in methods:\n",
        "        try:\n",
        "            result = method(text)\n",
        "            if validate_augmentation(text, result):\n",
        "                augmented.add(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying {method.__name__}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return list(augmented)\n",
        "\n",
        "def balance_samples(results_by_intent, target_samples_per_class, original_counts):\n",
        "    \"\"\"\n",
        "    Balance samples by intent with preference toward reaching TARGET_SAMPLES_PER_CLASS\n",
        "    \"\"\"\n",
        "    balanced_results = defaultdict(list)\n",
        "\n",
        "    for intent, samples in results_by_intent.items():\n",
        "        orig_count = original_counts.get(intent, 0)\n",
        "        current_count = len(samples)\n",
        "\n",
        "        # Keep all original data\n",
        "        original_data = samples[:orig_count]\n",
        "        balanced_results[intent].extend(original_data)\n",
        "\n",
        "        # Get augmented samples (everything after original data)\n",
        "        augmented_data = samples[orig_count:]\n",
        "\n",
        "        # Calculate how many we need\n",
        "        remaining_slots = target_samples_per_class - orig_count\n",
        "\n",
        "        if remaining_slots > 0:\n",
        "            # If we have enough augmented samples\n",
        "            if len(augmented_data) >= remaining_slots:\n",
        "                # Randomize selection\n",
        "                random.shuffle(augmented_data)\n",
        "                # Add what we need\n",
        "                balanced_results[intent].extend(augmented_data[:remaining_slots])\n",
        "            else:\n",
        "                # If we don't have enough, add all augmented samples\n",
        "                balanced_results[intent].extend(augmented_data)\n",
        "                # And duplicate some if needed (to reach closer to target)\n",
        "                shortage = remaining_slots - len(augmented_data)\n",
        "                if shortage > 0 and len(augmented_data) > 0:\n",
        "                    # Add duplicates of existing augmentations to help reach target\n",
        "                    extras = random.choices(augmented_data, k=min(shortage, len(augmented_data) * 2))\n",
        "                    balanced_results[intent].extend(extras)\n",
        "\n",
        "    return balanced_results\n",
        "\n",
        "def plot_distribution(data, title):\n",
        "    \"\"\"Plot distribution of samples by intent\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    data['intent'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Intent\")\n",
        "    plt.ylabel(\"Jumlah Sampel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =========[ MAIN PROCESS ]=========\n",
        "def main():\n",
        "    \"\"\"Main process for dataset augmentation\"\"\"\n",
        "    # Set up file paths based on chosen data type\n",
        "    if DATA_TYPE == \"train\":\n",
        "        OUTPUT_FILE = \"train.csv\"\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "    elif DATA_TYPE == \"val\":\n",
        "        # If different paths needed for validation\n",
        "        OUTPUT_FILE = \"val.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"val\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"val\")\n",
        "    elif DATA_TYPE == \"test\":\n",
        "        # If different paths needed for test\n",
        "        OUTPUT_FILE = \"test.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"test\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"test\")\n",
        "    else:\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_dataset(input_file)\n",
        "\n",
        "    # Initialize paraphrase model if enabled\n",
        "    model, tokenizer = initialize_paraphrase_model()\n",
        "\n",
        "    # Count original samples per intent\n",
        "    intent_counts = Counter(df['intent'])\n",
        "    print(\"Original class distribution:\")\n",
        "    for intent, count in intent_counts.items():\n",
        "        print(f\"  {intent}: {count}\")\n",
        "\n",
        "    # Calculate augmentation factors for balancing\n",
        "    augmentation_factors = {}\n",
        "    for intent, count in intent_counts.items():\n",
        "        if count >= TARGET_SAMPLES_PER_CLASS:\n",
        "            augmentation_factors[intent] = 1  # Minimum factor\n",
        "        else:\n",
        "            factor = max(1, min(10, TARGET_SAMPLES_PER_CLASS / count))  # Reduced max factor from 10 to 5\n",
        "            augmentation_factors[intent] = factor\n",
        "\n",
        "    print(\"\\nAugmentation factors:\")\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        print(f\"  {intent}: {factor:.2f}x\")\n",
        "\n",
        "    # Start augmentation process\n",
        "    print(\"Starting balanced augmentation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    augmented_results = defaultdict(list)\n",
        "    paraphrase_candidates = defaultdict(list)\n",
        "\n",
        "    # First, add all original data\n",
        "    for _, row in df.iterrows():\n",
        "        intent = row['intent']\n",
        "        text = row['text']\n",
        "        augmented_results[intent].append(text)\n",
        "\n",
        "    # Then determine augmentation targets for each intent\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        original_count = intent_counts[intent]\n",
        "        intent_df = df[df['intent'] == intent]\n",
        "\n",
        "        for _, row in tqdm(intent_df.iterrows(), desc=f\"Augmenting '{intent}'\", total=len(intent_df)):\n",
        "            text = row['text']\n",
        "\n",
        "            # Calculate needed augmentations for this sample\n",
        "            num_augmentations = max(\n",
        "                MIN_AUGMENTATIONS_PER_SAMPLE,\n",
        "                min(MAX_AUGMENTATIONS_PER_SAMPLE, int(factor * 2.0))  # Reduced multiplier from 2 to 1.5\n",
        "            )\n",
        "\n",
        "            # Regular augmentations\n",
        "            attempts = 0\n",
        "            augmentations_created = 0\n",
        "\n",
        "            while augmentations_created < num_augmentations and attempts < num_augmentations * 4:\n",
        "                attempts += 1\n",
        "                # More conservative intensity calculation\n",
        "                intensity = min(1.0, NOISE_INTENSITY + (factor - 1) * 0.2)  # Use NOISE_INTENSITY directly\n",
        "                aug_text = augment_text(text, intent, intensity)\n",
        "\n",
        "\n",
        "                if aug_text.lower() != text.lower() and validate_augmentation(text, aug_text):\n",
        "                    augmented_results[intent].append(aug_text)\n",
        "                    augmentations_created += 1\n",
        "\n",
        "            # For paraphrase model processing (batch later)\n",
        "            if USE_PARAPHRASE_MODEL and augmentations_created < num_augmentations:\n",
        "                paraphrase_candidates[intent].append(text)\n",
        "\n",
        "    # Batch paraphrase additional samples if needed\n",
        "    if USE_PARAPHRASE_MODEL and model is not None:\n",
        "        print(\"\\nApplying paraphrase model in batches...\")\n",
        "\n",
        "        for intent, texts in paraphrase_candidates.items():\n",
        "            # Skip intents that already have enough samples\n",
        "            if len(augmented_results[intent]) >= TARGET_SAMPLES_PER_CLASS:\n",
        "                continue\n",
        "\n",
        "            print(f\"  Processing {len(texts)} texts for intent '{intent}'\")\n",
        "\n",
        "            needed = TARGET_SAMPLES_PER_CLASS - len(augmented_results[intent])\n",
        "            # Process only what we need with a small buffer\n",
        "            process_count = min(len(texts), needed * 2)\n",
        "\n",
        "            # Get a random sample if there are many candidates\n",
        "            if len(texts) > process_count:\n",
        "                texts_to_process = random.sample(texts, process_count)\n",
        "            else:\n",
        "                texts_to_process = texts\n",
        "\n",
        "            # Process in batches\n",
        "            paraphrased = batch_paraphrase(model, tokenizer, texts_to_process, BATCH_SIZE)\n",
        "\n",
        "            # Add valid paraphrases\n",
        "            valid_count = 0\n",
        "            for orig, para in zip(texts_to_process, paraphrased):\n",
        "                if validate_augmentation(orig, para):\n",
        "                    augmented_results[intent].append(para)\n",
        "                    valid_count += 1\n",
        "\n",
        "                    # Stop if we have enough\n",
        "                    if len(augmented_results[intent]) >= TARGET_SAMPLES_PER_CLASS:\n",
        "                        break\n",
        "\n",
        "            print(f\"    Added {valid_count} valid paraphrases\")\n",
        "\n",
        "    # Balance the data\n",
        "    print(\"Balancing final dataset...\")\n",
        "    balanced_data = balance_samples(augmented_results, TARGET_SAMPLES_PER_CLASS, intent_counts)\n",
        "    # After all augmentation is done, check if classes are balanced\n",
        "    min_class_size = min(len(samples) for samples in balanced_data.values())\n",
        "\n",
        "    # If classes are not balanced, downsample the larger classes\n",
        "    for intent in balanced_data:\n",
        "        if len(balanced_data[intent]) > min_class_size:\n",
        "            # Keep all original data\n",
        "            orig_count = intent_counts.get(intent, 0)\n",
        "            original_data = balanced_data[intent][:orig_count]\n",
        "            augmented_data = balanced_data[intent][orig_count:]\n",
        "\n",
        "            # Randomly select augmented data to keep\n",
        "            needed = min_class_size - orig_count\n",
        "            if needed > 0 and augmented_data:\n",
        "                random.shuffle(augmented_data)\n",
        "                balanced_data[intent] = original_data + augmented_data[:needed]\n",
        "            else:\n",
        "                balanced_data[intent] = original_data[:min_class_size]\n",
        "    # Create final balanced dataframe\n",
        "    rows = []\n",
        "    for intent, texts in balanced_data.items():\n",
        "        for text in texts:\n",
        "            rows.append({\"intent\": intent, \"text\": text})\n",
        "\n",
        "    final_df = pd.DataFrame(rows)\n",
        "\n",
        "    # Print final statistics\n",
        "    print(\"\\nFinal dataset statistics:\")\n",
        "    final_counts = Counter(final_df['intent'])\n",
        "    for intent, count in final_counts.items():\n",
        "        orig = intent_counts.get(intent, 0)\n",
        "        added = count - orig\n",
        "        print(f\"  {intent}: {count} total ({orig} original + {added} augmented)\")\n",
        "\n",
        "    # Save to file\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nSaved balanced dataset to {output_file}\")\n",
        "\n",
        "    # Plot distributions\n",
        "    try:\n",
        "        print(\"Plotting class distributions...\")\n",
        "        plot_distribution(df, \"Original Distribution\")\n",
        "        plot_distribution(final_df, \"Augmented Distribution\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting distributions: {e}\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    original_total = len(df)\n",
        "    augmented_total = len(final_df)\n",
        "    time_taken = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Original samples: {original_total}\")\n",
        "    print(f\"  Final samples: {augmented_total}\")\n",
        "    print(f\"  Added samples: {augmented_total - original_total}\")\n",
        "    print(f\"  Augmentation ratio: {augmented_total / original_total:.2f}x\")\n",
        "    print(f\"  Processing time: {time_taken:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7cRi5Nhv_9S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Teks judul default\n",
        "# prompt: the model already trained and saved to drive, but can i test the model without running thee training.\n",
        "\n",
        "# Path ke folder model di Google Drive\n",
        "model_path = \"/content/drive/MyDrive/indobert_intent_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Load intent classes\n",
        "with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Load label encoder (jika diperlukan)\n",
        "with open(f\"{model_path}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "\n",
        "def predict_intent(text, model, tokenizer, intent_classes, device=None):\n",
        "    \"\"\"Memprediksi intent dari teks input\"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    predicted_intent = intent_classes[prediction]\n",
        "\n",
        "    # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "    top_k = 3\n",
        "    if len(intent_classes) < top_k:\n",
        "        top_k = len(intent_classes)\n",
        "\n",
        "    topk_values, topk_indices = torch.topk(probabilities, top_k, dim=1)\n",
        "    topk_intents = [(intent_classes[idx.item()], val.item()) for idx, val in zip(topk_indices[0], topk_values[0])]\n",
        "\n",
        "    return predicted_intent, confidence, topk_intents\n",
        "\n",
        "\n",
        "# Contoh penggunaan\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks untuk prediksi intent (ketik 'exit' untuk keluar): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        intent, confidence, topk = predict_intent(user_input, model, tokenizer, intent_classes)\n",
        "        print(f\"Intent terdeteksi: {intent} (confidence: {confidence:.4f})\")\n",
        "        print(\"Top 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(topk):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def balance_and_reduce_dataset(filepath, output_filepath):\n",
        "    \"\"\"\n",
        "    Balances and reduces a dataset, ensuring equal representation of each class.\n",
        "    Supports both .csv and .xlsx input files.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the input CSV or XLSX file.\n",
        "        output_filepath: Path to save the balanced and reduced dataset.\n",
        "    \"\"\"\n",
        "    # Check file extension\n",
        "    file_ext = os.path.splitext(filepath)[-1].lower()\n",
        "\n",
        "    if file_ext == '.xlsx':\n",
        "        try:\n",
        "            df = pd.read_excel(filepath)\n",
        "            # Convert to CSV first\n",
        "            temp_csv_path = filepath.replace('.xlsx', '_converted.csv')\n",
        "            df.to_csv(temp_csv_path, index=False)\n",
        "            print(f\"Converted XLSX to CSV: {temp_csv_path}\")\n",
        "            filepath = temp_csv_path  # Continue using the converted file\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading or converting Excel file: {e}\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"Error: Unable to parse the CSV file at {filepath}\")\n",
        "        return\n",
        "\n",
        "    # Validate required column\n",
        "    if 'intent' not in df.columns:\n",
        "        print(\"Error: 'intent' column not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Hitung setengah dari jumlah data tiap intent (min 1)\n",
        "    min_samples_per_class = max(1, df['intent'].value_counts().min() // 2)\n",
        "\n",
        "    balanced_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    for intent in df['intent'].unique():\n",
        "        intent_data = df[df['intent'] == intent]\n",
        "        if len(intent_data) < min_samples_per_class:\n",
        "            print(f\"Skipping intent '{intent}' due to insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        sampled_intent_data = intent_data.sample(n=min_samples_per_class, random_state=42)\n",
        "        balanced_df = pd.concat([balanced_df, sampled_intent_data], ignore_index=True)\n",
        "\n",
        "    if balanced_df.empty:\n",
        "        print(\"Resulting dataset is empty. No intents had enough data.\")\n",
        "        return\n",
        "\n",
        "    balanced_df.to_csv(output_filepath, index=False)\n",
        "    print(f\"Balanced and reduced dataset saved to {output_filepath}\")\n",
        "\n",
        "balance_and_reduce_dataset('/content/ChatbotPerpusBipa/train.xlsx', '/content/test.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "6__vZAEFwm0m",
        "outputId": "063513b2-bbab-4f6a-d1db-09b8b21c528d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted XLSX to CSV: /content/ChatbotPerpusBipa/train_converted.csv\n",
            "Balanced and reduced dataset saved to /content/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    (\"Halo, selamat pagi!\", \"greeting\"),\n",
        "    (\"Hai admin, apa kabar?\", \"greeting\"),\n",
        "    (\"Selamat siang, min!\", \"greeting\"),\n",
        "    (\"Halo, saya mau tanya.\", \"greeting\"),\n",
        "    (\"Permisi, boleh bertanya?\", \"greeting\"),\n",
        "    (\"Assalamualaikum\", \"greeting\"),\n",
        "    (\"Hallo kak, semoga harimu menyenangkan\", \"greeting\"),\n",
        "    (\"Selamat malam semuanya!\", \"greeting\"),\n",
        "    (\"Min, salam kenal ya\", \"greeting\"),\n",
        "    (\"Hai, aku pengguna baru nih\", \"greeting\"),\n",
        "\n",
        "    (\"Oke, saya setuju.\", \"confirm\"),\n",
        "    (\"Benar, itu yang saya maksud.\", \"confirm\"),\n",
        "    (\"Ya, saya menyetujui.\", \"confirm\"),\n",
        "    (\"Baik, lanjutkan saja.\", \"confirm\"),\n",
        "    (\"Saya sepakat dengan hal itu.\", \"confirm\"),\n",
        "    (\"Iya, saya setuju banget.\", \"confirm\"),\n",
        "    (\"Sudah sesuai kok.\", \"confirm\"),\n",
        "    (\"Silakan, itu benar.\", \"confirm\"),\n",
        "    (\"Aku menyetujuinya\", \"confirm\"),\n",
        "    (\"Jawaban itu tepat\", \"confirm\"),\n",
        "\n",
        "    (\"Tidak, saya tidak setuju.\", \"denied\"),\n",
        "    (\"Bukan, bukan itu maksud saya.\", \"denied\"),\n",
        "    (\"Maaf, saya kurang setuju.\", \"denied\"),\n",
        "    (\"Itu bukan yang saya cari.\", \"denied\"),\n",
        "    (\"Saya rasa itu salah.\", \"denied\"),\n",
        "    (\"Tidak sesuai dengan kebutuhan saya.\", \"denied\"),\n",
        "    (\"Bukan begitu, maaf.\", \"denied\"),\n",
        "    (\"Sepertinya bukan itu.\", \"denied\"),\n",
        "    (\"Saya menolak opsi tersebut.\", \"denied\"),\n",
        "    (\"Saya kurang setuju.\", \"denied\"),\n",
        "\n",
        "    (\"Terima kasih, sampai jumpa.\", \"goodbye\"),\n",
        "    (\"Oke, makasih ya min.\", \"goodbye\"),\n",
        "    (\"Dadah, sampai nanti.\", \"goodbye\"),\n",
        "    (\"Selamat tinggal.\", \"goodbye\"),\n",
        "    (\"Sampai bertemu kembali.\", \"goodbye\"),\n",
        "    (\"Saya pamit dulu ya.\", \"goodbye\"),\n",
        "    (\"Sekian dari saya, terima kasih.\", \"goodbye\"),\n",
        "    (\"Terima kasih atas bantuannya\", \"goodbye\"),\n",
        "    (\"Sampai ketemu lain waktu\", \"goodbye\"),\n",
        "    (\"Saya keluar dulu ya\", \"goodbye\"),\n",
        "\n",
        "    (\"Jam berapa perpustakaan buka?\", \"jam_layanan\"),\n",
        "    (\"Boleh tahu jam operasionalnya?\", \"jam_layanan\"),\n",
        "    (\"Library buka hari Sabtu nggak?\", \"jam_layanan\"),\n",
        "    (\"Perpustakaan tutup hari Minggu?\", \"jam_layanan\"),\n",
        "    (\"Jadwal buka perpustakaan apa ya?\", \"jam_layanan\"),\n",
        "    (\"Sampai jam berapa buka hari ini?\", \"jam_layanan\"),\n",
        "    (\"Jam layanan offline sampai kapan?\", \"jam_layanan\"),\n",
        "    (\"Ada libur operasional?\", \"jam_layanan\"),\n",
        "    (\"Hari libur nasional tetap buka?\", \"jam_layanan\"),\n",
        "    (\"Bisa info jam buka lengkapnya?\", \"jam_layanan\"),\n",
        "\n",
        "    (\"Ada buku tentang jaringan komputer?\", \"cari_buku\"),\n",
        "    (\"Saya cari buku algoritma dasar.\", \"cari_buku\"),\n",
        "    (\"Punya buku sejarah Indonesia?\", \"cari_buku\"),\n",
        "    (\"Buku tentang pemrograman Python ada?\", \"cari_buku\"),\n",
        "    (\"Saya mau pinjam buku filsafat.\", \"cari_buku\"),\n",
        "    (\"Ada daftar buku terbaru?\", \"cari_buku\"),\n",
        "    (\"Buku manajemen bisnis tersedia?\", \"cari_buku\"),\n",
        "    (\"Apakah ada buku referensi skripsi?\", \"cari_buku\"),\n",
        "    (\"Buku motivasi diri ada?\", \"cari_buku\"),\n",
        "    (\"Saya cari novel fiksi, ada?\", \"cari_buku\")\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"intent\"])\n",
        "df.to_csv(\"custom_test_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2bICI1U4QZLy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: train.csv the dataset has coloumn text and intent, scans the datasets, and check if there is any duplicate row on text in same intent, and clear it so there is only one. and print the results\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv('/content/test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found. Please upload the file to the Colab environment.\")\n",
        "    exit()\n",
        "\n",
        "# Remove duplicates based on 'text' and 'intent' columns, keeping the first occurrence\n",
        "df_deduplicated = df.drop_duplicates(subset=['text', 'intent'], keep='first')\n",
        "\n",
        "# Print the shape of the original and deduplicated DataFrames\n",
        "print(f\"Original DataFrame shape: {df.shape}\")\n",
        "print(f\"Deduplicated DataFrame shape: {df_deduplicated.shape}\")\n",
        "\n",
        "# Print the deduplicated DataFrame (optional)\n",
        "df_deduplicated\n",
        "\n",
        "# Save the deduplicated DataFrame to a new CSV file (optional)\n",
        "df_deduplicated.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klrC0Xom8Vot",
        "outputId": "7c87d1cb-617f-47ab-eb7a-f0756a935956"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame shape: (1500, 2)\n",
            "Deduplicated DataFrame shape: (446, 2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}